{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "006-Multi-layer-perceptron.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUPFEuSLtw2V",
        "colab_type": "text"
      },
      "source": [
        "# Multilayer Perceptron - MLP\n",
        "\n",
        "![alt text](https://miro.medium.com/max/1200/1*xxZXeKfVKTRqh54t10815A.jpeg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUY9l3s8tcbO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Import scikit-learn dataset library\n",
        "from sklearn import datasets\n",
        "\n",
        "#Load dataset\n",
        "cancer = datasets.load_breast_cancer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yd2VpZ24ugJA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "outputId": "0cc66902-b403-4a7c-e019-6f8f33c64d1e"
      },
      "source": [
        "print(cancer.target.shape)\n",
        "print(cancer.target)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(569,)\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 1 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0 0\n",
            " 1 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1\n",
            " 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 0 1\n",
            " 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 0 1 0\n",
            " 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 1\n",
            " 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1\n",
            " 1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0\n",
            " 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
            " 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1\n",
            " 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1\n",
            " 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 0 0\n",
            " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 0 0 0 0 0 0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qj-4HLev-V3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import train_test_split function\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split dataset into training set and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(cancer.data, \n",
        "                                                    cancer.target, \n",
        "                                                    test_size=0.3,\n",
        "                                                    random_state=109) # 70% training and 30% test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4PeMCO34xFL",
        "colab_type": "text"
      },
      "source": [
        "## Train and Predict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1N7WRdeYyNcW",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
        "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
        "       hidden_layer_sizes=(30, 30, 30), learning_rate='constant',\n",
        "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
        "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
        "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
        "       verbose=False, warm_start=False)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oF0V1NVgr1rv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f486d4f9-466b-4b42-b201-3a62c512eacb"
      },
      "source": [
        "help(MLPClassifier)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on class MLPClassifier in module sklearn.neural_network._multilayer_perceptron:\n",
            "\n",
            "class MLPClassifier(sklearn.base.ClassifierMixin, BaseMultilayerPerceptron)\n",
            " |  Multi-layer Perceptron classifier.\n",
            " |  \n",
            " |  This model optimizes the log-loss function using LBFGS or stochastic\n",
            " |  gradient descent.\n",
            " |  \n",
            " |  .. versionadded:: 0.18\n",
            " |  \n",
            " |  Parameters\n",
            " |  ----------\n",
            " |  hidden_layer_sizes : tuple, length = n_layers - 2, default=(100,)\n",
            " |      The ith element represents the number of neurons in the ith\n",
            " |      hidden layer.\n",
            " |  \n",
            " |  activation : {'identity', 'logistic', 'tanh', 'relu'}, default='relu'\n",
            " |      Activation function for the hidden layer.\n",
            " |  \n",
            " |      - 'identity', no-op activation, useful to implement linear bottleneck,\n",
            " |        returns f(x) = x\n",
            " |  \n",
            " |      - 'logistic', the logistic sigmoid function,\n",
            " |        returns f(x) = 1 / (1 + exp(-x)).\n",
            " |  \n",
            " |      - 'tanh', the hyperbolic tan function,\n",
            " |        returns f(x) = tanh(x).\n",
            " |  \n",
            " |      - 'relu', the rectified linear unit function,\n",
            " |        returns f(x) = max(0, x)\n",
            " |  \n",
            " |  solver : {'lbfgs', 'sgd', 'adam'}, default='adam'\n",
            " |      The solver for weight optimization.\n",
            " |  \n",
            " |      - 'lbfgs' is an optimizer in the family of quasi-Newton methods.\n",
            " |  \n",
            " |      - 'sgd' refers to stochastic gradient descent.\n",
            " |  \n",
            " |      - 'adam' refers to a stochastic gradient-based optimizer proposed\n",
            " |        by Kingma, Diederik, and Jimmy Ba\n",
            " |  \n",
            " |      Note: The default solver 'adam' works pretty well on relatively\n",
            " |      large datasets (with thousands of training samples or more) in terms of\n",
            " |      both training time and validation score.\n",
            " |      For small datasets, however, 'lbfgs' can converge faster and perform\n",
            " |      better.\n",
            " |  \n",
            " |  alpha : float, default=0.0001\n",
            " |      L2 penalty (regularization term) parameter.\n",
            " |  \n",
            " |  batch_size : int, default='auto'\n",
            " |      Size of minibatches for stochastic optimizers.\n",
            " |      If the solver is 'lbfgs', the classifier will not use minibatch.\n",
            " |      When set to \"auto\", `batch_size=min(200, n_samples)`\n",
            " |  \n",
            " |  learning_rate : {'constant', 'invscaling', 'adaptive'}, default='constant'\n",
            " |      Learning rate schedule for weight updates.\n",
            " |  \n",
            " |      - 'constant' is a constant learning rate given by\n",
            " |        'learning_rate_init'.\n",
            " |  \n",
            " |      - 'invscaling' gradually decreases the learning rate at each\n",
            " |        time step 't' using an inverse scaling exponent of 'power_t'.\n",
            " |        effective_learning_rate = learning_rate_init / pow(t, power_t)\n",
            " |  \n",
            " |      - 'adaptive' keeps the learning rate constant to\n",
            " |        'learning_rate_init' as long as training loss keeps decreasing.\n",
            " |        Each time two consecutive epochs fail to decrease training loss by at\n",
            " |        least tol, or fail to increase validation score by at least tol if\n",
            " |        'early_stopping' is on, the current learning rate is divided by 5.\n",
            " |  \n",
            " |      Only used when ``solver='sgd'``.\n",
            " |  \n",
            " |  learning_rate_init : double, default=0.001\n",
            " |      The initial learning rate used. It controls the step-size\n",
            " |      in updating the weights. Only used when solver='sgd' or 'adam'.\n",
            " |  \n",
            " |  power_t : double, default=0.5\n",
            " |      The exponent for inverse scaling learning rate.\n",
            " |      It is used in updating effective learning rate when the learning_rate\n",
            " |      is set to 'invscaling'. Only used when solver='sgd'.\n",
            " |  \n",
            " |  max_iter : int, default=200\n",
            " |      Maximum number of iterations. The solver iterates until convergence\n",
            " |      (determined by 'tol') or this number of iterations. For stochastic\n",
            " |      solvers ('sgd', 'adam'), note that this determines the number of epochs\n",
            " |      (how many times each data point will be used), not the number of\n",
            " |      gradient steps.\n",
            " |  \n",
            " |  shuffle : bool, default=True\n",
            " |      Whether to shuffle samples in each iteration. Only used when\n",
            " |      solver='sgd' or 'adam'.\n",
            " |  \n",
            " |  random_state : int, RandomState instance or None, default=None\n",
            " |      If int, random_state is the seed used by the random number generator;\n",
            " |      If RandomState instance, random_state is the random number generator;\n",
            " |      If None, the random number generator is the RandomState instance used\n",
            " |      by `np.random`.\n",
            " |  \n",
            " |  tol : float, default=1e-4\n",
            " |      Tolerance for the optimization. When the loss or score is not improving\n",
            " |      by at least ``tol`` for ``n_iter_no_change`` consecutive iterations,\n",
            " |      unless ``learning_rate`` is set to 'adaptive', convergence is\n",
            " |      considered to be reached and training stops.\n",
            " |  \n",
            " |  verbose : bool, default=False\n",
            " |      Whether to print progress messages to stdout.\n",
            " |  \n",
            " |  warm_start : bool, default=False\n",
            " |      When set to True, reuse the solution of the previous\n",
            " |      call to fit as initialization, otherwise, just erase the\n",
            " |      previous solution. See :term:`the Glossary <warm_start>`.\n",
            " |  \n",
            " |  momentum : float, default=0.9\n",
            " |      Momentum for gradient descent update. Should be between 0 and 1. Only\n",
            " |      used when solver='sgd'.\n",
            " |  \n",
            " |  nesterovs_momentum : boolean, default=True\n",
            " |      Whether to use Nesterov's momentum. Only used when solver='sgd' and\n",
            " |      momentum > 0.\n",
            " |  \n",
            " |  early_stopping : bool, default=False\n",
            " |      Whether to use early stopping to terminate training when validation\n",
            " |      score is not improving. If set to true, it will automatically set\n",
            " |      aside 10% of training data as validation and terminate training when\n",
            " |      validation score is not improving by at least tol for\n",
            " |      ``n_iter_no_change`` consecutive epochs. The split is stratified,\n",
            " |      except in a multilabel setting.\n",
            " |      Only effective when solver='sgd' or 'adam'\n",
            " |  \n",
            " |  validation_fraction : float, default=0.1\n",
            " |      The proportion of training data to set aside as validation set for\n",
            " |      early stopping. Must be between 0 and 1.\n",
            " |      Only used if early_stopping is True\n",
            " |  \n",
            " |  beta_1 : float, default=0.9\n",
            " |      Exponential decay rate for estimates of first moment vector in adam,\n",
            " |      should be in [0, 1). Only used when solver='adam'\n",
            " |  \n",
            " |  beta_2 : float, default=0.999\n",
            " |      Exponential decay rate for estimates of second moment vector in adam,\n",
            " |      should be in [0, 1). Only used when solver='adam'\n",
            " |  \n",
            " |  epsilon : float, default=1e-8\n",
            " |      Value for numerical stability in adam. Only used when solver='adam'\n",
            " |  \n",
            " |  n_iter_no_change : int, default=10\n",
            " |      Maximum number of epochs to not meet ``tol`` improvement.\n",
            " |      Only effective when solver='sgd' or 'adam'\n",
            " |  \n",
            " |      .. versionadded:: 0.20\n",
            " |  \n",
            " |  max_fun : int, default=15000\n",
            " |      Only used when solver='lbfgs'. Maximum number of loss function calls.\n",
            " |      The solver iterates until convergence (determined by 'tol'), number\n",
            " |      of iterations reaches max_iter, or this number of loss function calls.\n",
            " |      Note that number of loss function calls will be greater than or equal\n",
            " |      to the number of iterations for the `MLPClassifier`.\n",
            " |  \n",
            " |      .. versionadded:: 0.22\n",
            " |  \n",
            " |  Attributes\n",
            " |  ----------\n",
            " |  classes_ : ndarray or list of ndarray of shape (n_classes,)\n",
            " |      Class labels for each output.\n",
            " |  \n",
            " |  loss_ : float\n",
            " |      The current loss computed with the loss function.\n",
            " |  \n",
            " |  coefs_ : list, length n_layers - 1\n",
            " |      The ith element in the list represents the weight matrix corresponding\n",
            " |      to layer i.\n",
            " |  \n",
            " |  intercepts_ : list, length n_layers - 1\n",
            " |      The ith element in the list represents the bias vector corresponding to\n",
            " |      layer i + 1.\n",
            " |  \n",
            " |  n_iter_ : int,\n",
            " |      The number of iterations the solver has ran.\n",
            " |  \n",
            " |  n_layers_ : int\n",
            " |      Number of layers.\n",
            " |  \n",
            " |  n_outputs_ : int\n",
            " |      Number of outputs.\n",
            " |  \n",
            " |  out_activation_ : string\n",
            " |      Name of the output activation function.\n",
            " |  \n",
            " |  Notes\n",
            " |  -----\n",
            " |  MLPClassifier trains iteratively since at each time step\n",
            " |  the partial derivatives of the loss function with respect to the model\n",
            " |  parameters are computed to update the parameters.\n",
            " |  \n",
            " |  It can also have a regularization term added to the loss function\n",
            " |  that shrinks model parameters to prevent overfitting.\n",
            " |  \n",
            " |  This implementation works with data represented as dense numpy arrays or\n",
            " |  sparse scipy arrays of floating point values.\n",
            " |  \n",
            " |  References\n",
            " |  ----------\n",
            " |  Hinton, Geoffrey E.\n",
            " |      \"Connectionist learning procedures.\" Artificial intelligence 40.1\n",
            " |      (1989): 185-234.\n",
            " |  \n",
            " |  Glorot, Xavier, and Yoshua Bengio. \"Understanding the difficulty of\n",
            " |      training deep feedforward neural networks.\" International Conference\n",
            " |      on Artificial Intelligence and Statistics. 2010.\n",
            " |  \n",
            " |  He, Kaiming, et al. \"Delving deep into rectifiers: Surpassing human-level\n",
            " |      performance on imagenet classification.\" arXiv preprint\n",
            " |      arXiv:1502.01852 (2015).\n",
            " |  \n",
            " |  Kingma, Diederik, and Jimmy Ba. \"Adam: A method for stochastic\n",
            " |      optimization.\" arXiv preprint arXiv:1412.6980 (2014).\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      MLPClassifier\n",
            " |      sklearn.base.ClassifierMixin\n",
            " |      BaseMultilayerPerceptron\n",
            " |      sklearn.base.BaseEstimator\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, hidden_layer_sizes=(100,), activation='relu', solver='adam', alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10, max_fun=15000)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  fit(self, X, y)\n",
            " |      Fit the model to data matrix X and target(s) y.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : ndarray or sparse matrix of shape (n_samples, n_features)\n",
            " |          The input data.\n",
            " |      \n",
            " |      y : ndarray, shape (n_samples,) or (n_samples, n_outputs)\n",
            " |          The target values (class labels in classification, real numbers in\n",
            " |          regression).\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : returns a trained MLP model.\n",
            " |  \n",
            " |  predict(self, X)\n",
            " |      Predict using the multi-layer perceptron classifier\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
            " |          The input data.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      y : ndarray, shape (n_samples,) or (n_samples, n_classes)\n",
            " |          The predicted classes.\n",
            " |  \n",
            " |  predict_log_proba(self, X)\n",
            " |      Return the log of probability estimates.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : ndarray of shape (n_samples, n_features)\n",
            " |          The input data.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      log_y_prob : ndarray of shape (n_samples, n_classes)\n",
            " |          The predicted log-probability of the sample for each class\n",
            " |          in the model, where classes are ordered as they are in\n",
            " |          `self.classes_`. Equivalent to log(predict_proba(X))\n",
            " |  \n",
            " |  predict_proba(self, X)\n",
            " |      Probability estimates.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
            " |          The input data.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      y_prob : ndarray of shape (n_samples, n_classes)\n",
            " |          The predicted probability of the sample for each class in the\n",
            " |          model, where classes are ordered as they are in `self.classes_`.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors defined here:\n",
            " |  \n",
            " |  partial_fit\n",
            " |      Update the model with a single iteration over the given data.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
            " |          The input data.\n",
            " |      \n",
            " |      y : array-like, shape (n_samples,)\n",
            " |          The target values.\n",
            " |      \n",
            " |      classes : array, shape (n_classes), default None\n",
            " |          Classes across all calls to partial_fit.\n",
            " |          Can be obtained via `np.unique(y_all)`, where y_all is the\n",
            " |          target vector of the entire dataset.\n",
            " |          This argument is required for the first call to partial_fit\n",
            " |          and can be omitted in the subsequent calls.\n",
            " |          Note that y doesn't need to contain all labels in `classes`.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : returns a trained MLP model.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  __abstractmethods__ = frozenset()\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
            " |  \n",
            " |  score(self, X, y, sample_weight=None)\n",
            " |      Return the mean accuracy on the given test data and labels.\n",
            " |      \n",
            " |      In multi-label classification, this is the subset accuracy\n",
            " |      which is a harsh metric since you require for each sample that\n",
            " |      each label set be correctly predicted.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like of shape (n_samples, n_features)\n",
            " |          Test samples.\n",
            " |      \n",
            " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
            " |          True labels for X.\n",
            " |      \n",
            " |      sample_weight : array-like of shape (n_samples,), default=None\n",
            " |          Sample weights.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      score : float\n",
            " |          Mean accuracy of self.predict(X) wrt. y.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.base.BaseEstimator:\n",
            " |  \n",
            " |  __getstate__(self)\n",
            " |  \n",
            " |  __repr__(self, N_CHAR_MAX=700)\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  __setstate__(self, state)\n",
            " |  \n",
            " |  get_params(self, deep=True)\n",
            " |      Get parameters for this estimator.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      deep : bool, default=True\n",
            " |          If True, will return the parameters for this estimator and\n",
            " |          contained subobjects that are estimators.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      params : mapping of string to any\n",
            " |          Parameter names mapped to their values.\n",
            " |  \n",
            " |  set_params(self, **params)\n",
            " |      Set the parameters of this estimator.\n",
            " |      \n",
            " |      The method works on simple estimators as well as on nested objects\n",
            " |      (such as pipelines). The latter have parameters of the form\n",
            " |      ``<component>__<parameter>`` so that it's possible to update each\n",
            " |      component of a nested object.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      **params : dict\n",
            " |          Estimator parameters.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : object\n",
            " |          Estimator instance.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULQgWI_LqAzY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "41ee6d91-9270-4683-a3a6-306d57136b45"
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "clf = MLPClassifier(activation='logistic',\n",
        "                    hidden_layer_sizes=(100,500,10),\n",
        "                    solver='adam',             \n",
        "                    tol = 0.00001,\n",
        "                    validation_fraction = 0.2,        \n",
        "                    max_iter=2000, verbose=True)\n",
        "clf.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.66147107\n",
            "Iteration 2, loss = 0.65876958\n",
            "Iteration 3, loss = 0.65735914\n",
            "Iteration 4, loss = 0.65498796\n",
            "Iteration 5, loss = 0.65080424\n",
            "Iteration 6, loss = 0.64717843\n",
            "Iteration 7, loss = 0.64298530\n",
            "Iteration 8, loss = 0.63613500\n",
            "Iteration 9, loss = 0.62890944\n",
            "Iteration 10, loss = 0.62129990\n",
            "Iteration 11, loss = 0.61221713\n",
            "Iteration 12, loss = 0.60164762\n",
            "Iteration 13, loss = 0.58890299\n",
            "Iteration 14, loss = 0.57365404\n",
            "Iteration 15, loss = 0.55640987\n",
            "Iteration 16, loss = 0.53787753\n",
            "Iteration 17, loss = 0.51861048\n",
            "Iteration 18, loss = 0.50023033\n",
            "Iteration 19, loss = 0.48040316\n",
            "Iteration 20, loss = 0.46159663\n",
            "Iteration 21, loss = 0.44217146\n",
            "Iteration 22, loss = 0.42517150\n",
            "Iteration 23, loss = 0.41002533\n",
            "Iteration 24, loss = 0.39637936\n",
            "Iteration 25, loss = 0.38358242\n",
            "Iteration 26, loss = 0.37274501\n",
            "Iteration 27, loss = 0.36325922\n",
            "Iteration 28, loss = 0.35736321\n",
            "Iteration 29, loss = 0.34773163\n",
            "Iteration 30, loss = 0.34630317\n",
            "Iteration 31, loss = 0.33909144\n",
            "Iteration 32, loss = 0.33387864\n",
            "Iteration 33, loss = 0.33296535\n",
            "Iteration 34, loss = 0.32570312\n",
            "Iteration 35, loss = 0.32177479\n",
            "Iteration 36, loss = 0.31835838\n",
            "Iteration 37, loss = 0.31477996\n",
            "Iteration 38, loss = 0.31027838\n",
            "Iteration 39, loss = 0.30847737\n",
            "Iteration 40, loss = 0.30425224\n",
            "Iteration 41, loss = 0.30500823\n",
            "Iteration 42, loss = 0.30058921\n",
            "Iteration 43, loss = 0.29878893\n",
            "Iteration 44, loss = 0.29439676\n",
            "Iteration 45, loss = 0.30047746\n",
            "Iteration 46, loss = 0.29507204\n",
            "Iteration 47, loss = 0.29367715\n",
            "Iteration 48, loss = 0.29237271\n",
            "Iteration 49, loss = 0.29024933\n",
            "Iteration 50, loss = 0.28483085\n",
            "Iteration 51, loss = 0.28746798\n",
            "Iteration 52, loss = 0.28083579\n",
            "Iteration 53, loss = 0.28372936\n",
            "Iteration 54, loss = 0.27686466\n",
            "Iteration 55, loss = 0.27877852\n",
            "Iteration 56, loss = 0.27449022\n",
            "Iteration 57, loss = 0.27460905\n",
            "Iteration 58, loss = 0.26977507\n",
            "Iteration 59, loss = 0.27254277\n",
            "Iteration 60, loss = 0.26538128\n",
            "Iteration 61, loss = 0.26845017\n",
            "Iteration 62, loss = 0.26362810\n",
            "Iteration 63, loss = 0.26377797\n",
            "Iteration 64, loss = 0.25997861\n",
            "Iteration 65, loss = 0.25867937\n",
            "Iteration 66, loss = 0.26110415\n",
            "Iteration 67, loss = 0.26414538\n",
            "Iteration 68, loss = 0.25375286\n",
            "Iteration 69, loss = 0.25734140\n",
            "Iteration 70, loss = 0.25079255\n",
            "Iteration 71, loss = 0.25393086\n",
            "Iteration 72, loss = 0.25432976\n",
            "Iteration 73, loss = 0.24903606\n",
            "Iteration 74, loss = 0.25794175\n",
            "Iteration 75, loss = 0.24969553\n",
            "Iteration 76, loss = 0.25636988\n",
            "Iteration 77, loss = 0.24713881\n",
            "Iteration 78, loss = 0.24655623\n",
            "Iteration 79, loss = 0.24689091\n",
            "Iteration 80, loss = 0.24835780\n",
            "Iteration 81, loss = 0.24159246\n",
            "Iteration 82, loss = 0.23516603\n",
            "Iteration 83, loss = 0.23443063\n",
            "Iteration 84, loss = 0.23281978\n",
            "Iteration 85, loss = 0.23326884\n",
            "Iteration 86, loss = 0.23039901\n",
            "Iteration 87, loss = 0.23004906\n",
            "Iteration 88, loss = 0.22844218\n",
            "Iteration 89, loss = 0.22662079\n",
            "Iteration 90, loss = 0.22658905\n",
            "Iteration 91, loss = 0.22465340\n",
            "Iteration 92, loss = 0.22606884\n",
            "Iteration 93, loss = 0.22261750\n",
            "Iteration 94, loss = 0.22250002\n",
            "Iteration 95, loss = 0.22347127\n",
            "Iteration 96, loss = 0.22106762\n",
            "Iteration 97, loss = 0.22746152\n",
            "Iteration 98, loss = 0.21770030\n",
            "Iteration 99, loss = 0.22562396\n",
            "Iteration 100, loss = 0.21821494\n",
            "Iteration 101, loss = 0.21901303\n",
            "Iteration 102, loss = 0.23759283\n",
            "Iteration 103, loss = 0.22577947\n",
            "Iteration 104, loss = 0.21931951\n",
            "Iteration 105, loss = 0.21755362\n",
            "Iteration 106, loss = 0.21503281\n",
            "Iteration 107, loss = 0.21427846\n",
            "Iteration 108, loss = 0.20840731\n",
            "Iteration 109, loss = 0.21513203\n",
            "Iteration 110, loss = 0.20595436\n",
            "Iteration 111, loss = 0.21184484\n",
            "Iteration 112, loss = 0.21069749\n",
            "Iteration 113, loss = 0.20596257\n",
            "Iteration 114, loss = 0.21870327\n",
            "Iteration 115, loss = 0.20214757\n",
            "Iteration 116, loss = 0.21085293\n",
            "Iteration 117, loss = 0.20276651\n",
            "Iteration 118, loss = 0.21301604\n",
            "Iteration 119, loss = 0.20037672\n",
            "Iteration 120, loss = 0.20053192\n",
            "Iteration 121, loss = 0.19895484\n",
            "Iteration 122, loss = 0.20614288\n",
            "Iteration 123, loss = 0.19657178\n",
            "Iteration 124, loss = 0.19785557\n",
            "Iteration 125, loss = 0.19475234\n",
            "Iteration 126, loss = 0.20526239\n",
            "Iteration 127, loss = 0.19720925\n",
            "Iteration 128, loss = 0.19930818\n",
            "Iteration 129, loss = 0.19397949\n",
            "Iteration 130, loss = 0.19188083\n",
            "Iteration 131, loss = 0.19163147\n",
            "Iteration 132, loss = 0.19309440\n",
            "Iteration 133, loss = 0.19131309\n",
            "Iteration 134, loss = 0.18987167\n",
            "Iteration 135, loss = 0.19420734\n",
            "Iteration 136, loss = 0.18627701\n",
            "Iteration 137, loss = 0.19640138\n",
            "Iteration 138, loss = 0.19212568\n",
            "Iteration 139, loss = 0.18746677\n",
            "Iteration 140, loss = 0.19478314\n",
            "Iteration 141, loss = 0.18410433\n",
            "Iteration 142, loss = 0.18512943\n",
            "Iteration 143, loss = 0.19835279\n",
            "Iteration 144, loss = 0.19964508\n",
            "Iteration 145, loss = 0.18076359\n",
            "Iteration 146, loss = 0.19799724\n",
            "Iteration 147, loss = 0.18177050\n",
            "Iteration 148, loss = 0.20124021\n",
            "Iteration 149, loss = 0.17737836\n",
            "Iteration 150, loss = 0.20417410\n",
            "Iteration 151, loss = 0.19730199\n",
            "Iteration 152, loss = 0.19845440\n",
            "Iteration 153, loss = 0.18781409\n",
            "Iteration 154, loss = 0.18131255\n",
            "Iteration 155, loss = 0.18153418\n",
            "Iteration 156, loss = 0.17686459\n",
            "Iteration 157, loss = 0.17596235\n",
            "Iteration 158, loss = 0.17536125\n",
            "Iteration 159, loss = 0.17778400\n",
            "Iteration 160, loss = 0.17781840\n",
            "Iteration 161, loss = 0.17504017\n",
            "Iteration 162, loss = 0.17493184\n",
            "Iteration 163, loss = 0.17756555\n",
            "Iteration 164, loss = 0.17359512\n",
            "Iteration 165, loss = 0.17317393\n",
            "Iteration 166, loss = 0.17164222\n",
            "Iteration 167, loss = 0.17181827\n",
            "Iteration 168, loss = 0.17598192\n",
            "Iteration 169, loss = 0.16777031\n",
            "Iteration 170, loss = 0.17728144\n",
            "Iteration 171, loss = 0.17246400\n",
            "Iteration 172, loss = 0.17391116\n",
            "Iteration 173, loss = 0.17100541\n",
            "Iteration 174, loss = 0.17042567\n",
            "Iteration 175, loss = 0.16926171\n",
            "Iteration 176, loss = 0.17317346\n",
            "Iteration 177, loss = 0.16339248\n",
            "Iteration 178, loss = 0.17392656\n",
            "Iteration 179, loss = 0.16623946\n",
            "Iteration 180, loss = 0.16979258\n",
            "Iteration 181, loss = 0.17031870\n",
            "Iteration 182, loss = 0.16254880\n",
            "Iteration 183, loss = 0.16549067\n",
            "Iteration 184, loss = 0.16562526\n",
            "Iteration 185, loss = 0.17241861\n",
            "Iteration 186, loss = 0.16744712\n",
            "Iteration 187, loss = 0.17087248\n",
            "Iteration 188, loss = 0.16641154\n",
            "Iteration 189, loss = 0.16349244\n",
            "Iteration 190, loss = 0.15790793\n",
            "Iteration 191, loss = 0.16262041\n",
            "Iteration 192, loss = 0.15991591\n",
            "Iteration 193, loss = 0.16075444\n",
            "Iteration 194, loss = 0.16080681\n",
            "Iteration 195, loss = 0.15572799\n",
            "Iteration 196, loss = 0.16264952\n",
            "Iteration 197, loss = 0.15701047\n",
            "Iteration 198, loss = 0.15746026\n",
            "Iteration 199, loss = 0.15555675\n",
            "Iteration 200, loss = 0.15405059\n",
            "Iteration 201, loss = 0.15885168\n",
            "Iteration 202, loss = 0.15842040\n",
            "Iteration 203, loss = 0.15743938\n",
            "Iteration 204, loss = 0.15864285\n",
            "Iteration 205, loss = 0.15724477\n",
            "Iteration 206, loss = 0.16374937\n",
            "Iteration 207, loss = 0.15423168\n",
            "Iteration 208, loss = 0.16076721\n",
            "Iteration 209, loss = 0.15699778\n",
            "Iteration 210, loss = 0.15237591\n",
            "Iteration 211, loss = 0.15113099\n",
            "Iteration 212, loss = 0.15389910\n",
            "Iteration 213, loss = 0.15838429\n",
            "Iteration 214, loss = 0.14870005\n",
            "Iteration 215, loss = 0.15388659\n",
            "Iteration 216, loss = 0.16828929\n",
            "Iteration 217, loss = 0.14312337\n",
            "Iteration 218, loss = 0.15268501\n",
            "Iteration 219, loss = 0.14816683\n",
            "Iteration 220, loss = 0.14340417\n",
            "Iteration 221, loss = 0.14215168\n",
            "Iteration 222, loss = 0.14413815\n",
            "Iteration 223, loss = 0.14126756\n",
            "Iteration 224, loss = 0.14713004\n",
            "Iteration 225, loss = 0.13890051\n",
            "Iteration 226, loss = 0.14094604\n",
            "Iteration 227, loss = 0.14084665\n",
            "Iteration 228, loss = 0.13783569\n",
            "Iteration 229, loss = 0.13808937\n",
            "Iteration 230, loss = 0.14095174\n",
            "Iteration 231, loss = 0.13893039\n",
            "Iteration 232, loss = 0.13788816\n",
            "Iteration 233, loss = 0.13506089\n",
            "Iteration 234, loss = 0.13638831\n",
            "Iteration 235, loss = 0.13324473\n",
            "Iteration 236, loss = 0.14349396\n",
            "Iteration 237, loss = 0.13122019\n",
            "Iteration 238, loss = 0.13388437\n",
            "Iteration 239, loss = 0.14032235\n",
            "Iteration 240, loss = 0.13688221\n",
            "Iteration 241, loss = 0.13404280\n",
            "Iteration 242, loss = 0.14447451\n",
            "Iteration 243, loss = 0.14671690\n",
            "Iteration 244, loss = 0.13275940\n",
            "Iteration 245, loss = 0.13940521\n",
            "Iteration 246, loss = 0.13995566\n",
            "Iteration 247, loss = 0.13959708\n",
            "Iteration 248, loss = 0.12636320\n",
            "Iteration 249, loss = 0.14439574\n",
            "Iteration 250, loss = 0.13143033\n",
            "Iteration 251, loss = 0.13438064\n",
            "Iteration 252, loss = 0.13107580\n",
            "Iteration 253, loss = 0.12440368\n",
            "Iteration 254, loss = 0.12437077\n",
            "Iteration 255, loss = 0.12260696\n",
            "Iteration 256, loss = 0.12158368\n",
            "Iteration 257, loss = 0.12429722\n",
            "Iteration 258, loss = 0.12172699\n",
            "Iteration 259, loss = 0.11969933\n",
            "Iteration 260, loss = 0.12281043\n",
            "Iteration 261, loss = 0.13406371\n",
            "Iteration 262, loss = 0.13634584\n",
            "Iteration 263, loss = 0.14092800\n",
            "Iteration 264, loss = 0.11569880\n",
            "Iteration 265, loss = 0.14586469\n",
            "Iteration 266, loss = 0.13029878\n",
            "Iteration 267, loss = 0.15445024\n",
            "Iteration 268, loss = 0.14145231\n",
            "Iteration 269, loss = 0.13806719\n",
            "Iteration 270, loss = 0.14103362\n",
            "Iteration 271, loss = 0.13609998\n",
            "Iteration 272, loss = 0.13106704\n",
            "Iteration 273, loss = 0.12732528\n",
            "Iteration 274, loss = 0.13427206\n",
            "Iteration 275, loss = 0.13658662\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='logistic', alpha=0.0001, batch_size='auto',\n",
              "              beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
              "              hidden_layer_sizes=(100, 500, 10), learning_rate='constant',\n",
              "              learning_rate_init=0.001, max_fun=15000, max_iter=2000,\n",
              "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
              "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
              "              tol=1e-05, validation_fraction=0.2, verbose=True,\n",
              "              warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1DpKJOSvtFi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2508e57b-b2a5-42c5-f51c-1ef3b93ac080"
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "clf = MLPClassifier(solver='adam', hidden_layer_sizes=(10), random_state=1, verbose=True)\n",
        "#clf = MLPClassifier(activation='logistic',\n",
        "#                    hidden_layer_sizes=(100,500,10),\n",
        "#                    solver='adam',             \n",
        "#                    tol = 0.00001,\n",
        "#                    validation_fraction = 0.2,        \n",
        "#                    max_iter=2000, verbose=True)\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "neural_output = clf.predict(X_test)\n",
        "print(accuracy_score(y_test, neural_output))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.87378527\n",
            "Iteration 3, loss = 0.70910193\n",
            "Iteration 4, loss = 0.70969458\n",
            "Iteration 5, loss = 0.70960079\n",
            "Iteration 6, loss = 0.70931514\n",
            "Iteration 7, loss = 0.70897305\n",
            "Iteration 8, loss = 0.70855273\n",
            "Iteration 9, loss = 0.70815045\n",
            "Iteration 10, loss = 0.70775779\n",
            "Iteration 11, loss = 0.70733859\n",
            "Iteration 12, loss = 0.70690066\n",
            "Iteration 13, loss = 0.70648611\n",
            "Iteration 14, loss = 0.70605900\n",
            "Iteration 15, loss = 0.70560112\n",
            "Iteration 16, loss = 0.70521141\n",
            "Iteration 17, loss = 0.70476405\n",
            "Iteration 18, loss = 0.70433650\n",
            "Iteration 19, loss = 0.70393214\n",
            "Iteration 20, loss = 0.70347606\n",
            "Iteration 21, loss = 0.70309089\n",
            "Iteration 22, loss = 0.70264854\n",
            "Iteration 23, loss = 0.70222980\n",
            "Iteration 24, loss = 0.70183547\n",
            "Iteration 25, loss = 0.70141852\n",
            "Iteration 26, loss = 0.70100492\n",
            "Iteration 27, loss = 0.70065969\n",
            "Iteration 28, loss = 0.70023399\n",
            "Iteration 29, loss = 0.69982547\n",
            "Iteration 30, loss = 0.69944139\n",
            "Iteration 31, loss = 0.69908185\n",
            "Iteration 32, loss = 0.69866764\n",
            "Iteration 33, loss = 0.69830435\n",
            "Iteration 34, loss = 0.69793100\n",
            "Iteration 35, loss = 0.69755859\n",
            "Iteration 36, loss = 0.69717722\n",
            "Iteration 37, loss = 0.69682376\n",
            "Iteration 38, loss = 0.69645942\n",
            "Iteration 39, loss = 0.69611774\n",
            "Iteration 40, loss = 0.69573843\n",
            "Iteration 41, loss = 0.69541990\n",
            "Iteration 42, loss = 0.69509456\n",
            "Iteration 43, loss = 0.69474466\n",
            "Iteration 44, loss = 0.69440558\n",
            "Iteration 45, loss = 0.69405980\n",
            "Iteration 46, loss = 0.69373273\n",
            "Iteration 47, loss = 0.69342487\n",
            "Iteration 48, loss = 0.69309500\n",
            "Iteration 49, loss = 0.69277530\n",
            "Iteration 50, loss = 0.69244248\n",
            "Iteration 51, loss = 0.69213192\n",
            "Iteration 52, loss = 0.69180651\n",
            "Iteration 53, loss = 0.69151408\n",
            "Iteration 54, loss = 0.69118252\n",
            "Iteration 55, loss = 0.69091631\n",
            "Iteration 56, loss = 0.69061900\n",
            "Iteration 57, loss = 0.69031194\n",
            "Iteration 58, loss = 0.69003278\n",
            "Iteration 59, loss = 0.68971555\n",
            "Iteration 60, loss = 0.68943476\n",
            "Iteration 61, loss = 0.68919684\n",
            "Iteration 62, loss = 0.68886337\n",
            "Iteration 63, loss = 0.68860014\n",
            "Iteration 64, loss = 0.68831547\n",
            "Iteration 65, loss = 0.68811107\n",
            "Iteration 66, loss = 0.68776476\n",
            "Iteration 67, loss = 0.68754436\n",
            "Iteration 68, loss = 0.68726108\n",
            "Iteration 69, loss = 0.68699940\n",
            "Iteration 70, loss = 0.68674538\n",
            "Iteration 71, loss = 0.68647220\n",
            "Iteration 72, loss = 0.68622762\n",
            "Iteration 73, loss = 0.68597073\n",
            "Iteration 74, loss = 0.68574510\n",
            "Iteration 75, loss = 0.68548955\n",
            "Iteration 76, loss = 0.68525053\n",
            "Iteration 77, loss = 0.68500135\n",
            "Iteration 78, loss = 0.68474462\n",
            "Iteration 79, loss = 0.68453131\n",
            "Iteration 80, loss = 0.68427871\n",
            "Iteration 81, loss = 0.68407163\n",
            "Iteration 82, loss = 0.68383686\n",
            "Iteration 83, loss = 0.68360370\n",
            "Iteration 84, loss = 0.68339365\n",
            "Iteration 85, loss = 0.68316341\n",
            "Iteration 86, loss = 0.68294816\n",
            "Iteration 87, loss = 0.68274946\n",
            "Iteration 88, loss = 0.68251336\n",
            "Iteration 89, loss = 0.68230721\n",
            "Iteration 90, loss = 0.68207364\n",
            "Iteration 91, loss = 0.68188467\n",
            "Iteration 92, loss = 0.68165885\n",
            "Iteration 93, loss = 0.68146504\n",
            "Iteration 94, loss = 0.68128863\n",
            "Iteration 95, loss = 0.68106180\n",
            "Iteration 96, loss = 0.68086058\n",
            "Iteration 97, loss = 0.68068533\n",
            "Iteration 98, loss = 0.68047365\n",
            "Iteration 99, loss = 0.68029530\n",
            "Iteration 100, loss = 0.68008494\n",
            "Iteration 101, loss = 0.67991073\n",
            "Iteration 102, loss = 0.67972385\n",
            "Iteration 103, loss = 0.67953484\n",
            "Iteration 104, loss = 0.67934147\n",
            "Iteration 105, loss = 0.67919931\n",
            "Iteration 106, loss = 0.67900416\n",
            "Iteration 107, loss = 0.67879868\n",
            "Iteration 108, loss = 0.67865182\n",
            "Iteration 109, loss = 0.67847891\n",
            "Iteration 110, loss = 0.67828566\n",
            "Iteration 111, loss = 0.67811965\n",
            "Iteration 112, loss = 0.67795836\n",
            "Iteration 113, loss = 0.67779993\n",
            "Iteration 114, loss = 0.67762016\n",
            "Iteration 115, loss = 0.67747390\n",
            "Iteration 116, loss = 0.67730250\n",
            "Iteration 117, loss = 0.67712871\n",
            "Iteration 118, loss = 0.67697792\n",
            "Iteration 119, loss = 0.67682943\n",
            "Iteration 120, loss = 0.67667198\n",
            "Iteration 121, loss = 0.67652399\n",
            "Iteration 122, loss = 0.67637694\n",
            "Iteration 123, loss = 0.67620325\n",
            "Iteration 124, loss = 0.67606654\n",
            "Iteration 125, loss = 0.67593476\n",
            "Iteration 126, loss = 0.67575808\n",
            "Iteration 127, loss = 0.67563270\n",
            "Iteration 128, loss = 0.67547569\n",
            "Iteration 129, loss = 0.67534914\n",
            "Iteration 130, loss = 0.67519939\n",
            "Iteration 131, loss = 0.67505683\n",
            "Iteration 132, loss = 0.67492390\n",
            "Iteration 133, loss = 0.67479322\n",
            "Iteration 134, loss = 0.67466147\n",
            "Iteration 135, loss = 0.67453257\n",
            "Iteration 136, loss = 0.67438543\n",
            "Iteration 137, loss = 0.67424782\n",
            "Iteration 138, loss = 0.67412624\n",
            "Iteration 139, loss = 0.67400343\n",
            "Iteration 140, loss = 0.67388290\n",
            "Iteration 141, loss = 0.67374670\n",
            "Iteration 142, loss = 0.67362177\n",
            "Iteration 143, loss = 0.67350386\n",
            "Iteration 144, loss = 0.67339164\n",
            "Iteration 145, loss = 0.67327653\n",
            "Iteration 146, loss = 0.67314372\n",
            "Iteration 147, loss = 0.67302689\n",
            "Iteration 148, loss = 0.67290024\n",
            "Iteration 149, loss = 0.67278311\n",
            "Iteration 150, loss = 0.67268025\n",
            "Iteration 151, loss = 0.67256148\n",
            "Iteration 152, loss = 0.67245736\n",
            "Iteration 153, loss = 0.67233867\n",
            "Iteration 154, loss = 0.67223190\n",
            "Iteration 155, loss = 0.67211955\n",
            "Iteration 156, loss = 0.67201824\n",
            "Iteration 157, loss = 0.67189771\n",
            "Iteration 158, loss = 0.67181365\n",
            "Iteration 159, loss = 0.67170160\n",
            "Iteration 160, loss = 0.67159241\n",
            "Iteration 161, loss = 0.67148714\n",
            "Iteration 162, loss = 0.67139571\n",
            "Iteration 163, loss = 0.67128782\n",
            "Iteration 164, loss = 0.67118306\n",
            "Iteration 165, loss = 0.67108860\n",
            "Iteration 166, loss = 0.67099453\n",
            "Iteration 167, loss = 0.67089944\n",
            "Iteration 168, loss = 0.67080211\n",
            "Iteration 169, loss = 0.67071380\n",
            "Iteration 170, loss = 0.67060888\n",
            "Iteration 171, loss = 0.67051597\n",
            "Iteration 172, loss = 0.67042409\n",
            "Iteration 173, loss = 0.67034396\n",
            "Iteration 174, loss = 0.67025631\n",
            "Iteration 175, loss = 0.67016452\n",
            "Iteration 176, loss = 0.67007421\n",
            "Iteration 177, loss = 0.66997322\n",
            "Iteration 178, loss = 0.66989731\n",
            "Iteration 179, loss = 0.66980622\n",
            "Iteration 180, loss = 0.66973025\n",
            "Iteration 181, loss = 0.66965345\n",
            "Iteration 182, loss = 0.66956179\n",
            "Iteration 183, loss = 0.66947170\n",
            "Iteration 184, loss = 0.66939644\n",
            "Iteration 185, loss = 0.66931757\n",
            "Iteration 186, loss = 0.66923681\n",
            "Iteration 187, loss = 0.66916066\n",
            "Iteration 188, loss = 0.66907881\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "sgd\n",
            "0.631578947368421\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fhxo6J_yykip",
        "colab_type": "text"
      },
      "source": [
        "## Plot learning curve"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rK_LYawgymmO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "f34a0647-5b40-4ab6-93ca-07d69806f43d"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.DataFrame(clf.loss_curve_).plot()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f3c85b82358>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUuWh4nZ0dSN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "8e9fbbaa-acda-4b48-cafa-813a5ea2de3d"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.title('Learning curves', fontsize = 18)\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.plot(clf.loss_curve_, label = 'loss')\n",
        "plt.ylabel('Loss', fontsize = 14)\n",
        "plt.xlabel('epochs', fontsize = 14)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No handles with labels found to put in legend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqISUrvpwIFk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "outputId": "86fdd52c-7902-4e77-d367-e2e565e63914"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "for i in [10, 200, 300, 500]:\n",
        "  clf = MLPClassifier(solver='adam', hidden_layer_sizes=(i), random_state=1)\n",
        "  clf.fit(X_train, y_train)\n",
        "  \n",
        "  neural_output = clf.predict(X_test)\n",
        "  print('\\nadam,', 'hidden', i)\n",
        "  print(accuracy_score(y_test, neural_output))\n",
        "\n",
        "#clf = MLPClassifier(activation='logistic',\n",
        "#                    hidden_layer_sizes=(100,500,10),\n",
        "#                    solver='adam',             \n",
        "#                    tol = 0.00001,\n",
        "#                    validation_fraction = 0.2,        \n",
        "#                    max_iter=2000, verbose=True)  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "adam, hidden 10\n",
            "0.631578947368421\n",
            "\n",
            "adam, hidden 200\n",
            "0.9824561403508771\n",
            "\n",
            "adam, hidden 300\n",
            "0.9649122807017544\n",
            "\n",
            "adam, hidden 500\n",
            "0.9649122807017544\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtUrwgsUwdcV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "241f158a-7ed5-41b9-8c25-45861e0d90a6"
      },
      "source": [
        "for i in [10, 200, 300]:\n",
        "  clf = MLPClassifier(solver='adam', hidden_layer_sizes=(i), random_state=1)\n",
        "  clf.fit(X_train, y_train)\n",
        "  \n",
        "  neural_output = clf.predict(X_test)\n",
        "  print('\\nsgd,', 'hidden', i)\n",
        "  print(accuracy_score(y_test, neural_output))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "sgd, hidden 10\n",
            "0.631578947368421\n",
            "\n",
            "sgd, hidden 200\n",
            "0.9824561403508771\n",
            "\n",
            "sgd, hidden 300\n",
            "0.9649122807017544\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYXVEQz8xMlx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "5013123c-2f79-4d08-8e27-a39d58b6ecbe"
      },
      "source": [
        "proba_output = clf.predict_proba(X_test)\n",
        "print(proba_output[0:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.08722789e-01 8.91277211e-01]\n",
            " [1.99169405e-02 9.80083060e-01]\n",
            " [9.99999997e-01 3.15383864e-09]\n",
            " [1.00000000e+00 2.82881002e-12]\n",
            " [4.88133441e-01 5.11866559e-01]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVJWnoqlwIAh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "cfa84ad9-fd5c-4c4e-b14a-b34e97ec77f7"
      },
      "source": [
        "output = clf.predict(X_test)\n",
        "print(output[0:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 1 0 0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKuAMiVfxaz5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "outputId": "7e4fc4ef-34cb-4ab2-cfd7-df2d4ed9fb78"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "print(classification_report(y_test,neural_output))\n",
        "print('Confusion matrix')\n",
        "print(confusion_matrix(y_test,neural_output))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.90      0.95        63\n",
            "           1       0.95      1.00      0.97       108\n",
            "\n",
            "    accuracy                           0.96       171\n",
            "   macro avg       0.97      0.95      0.96       171\n",
            "weighted avg       0.97      0.96      0.96       171\n",
            "\n",
            "Confusion matrix\n",
            "[[ 57   6]\n",
            " [  0 108]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbNnXgik5MQu",
        "colab_type": "text"
      },
      "source": [
        "# MLP using Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCdOdR3xx0LZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1be48806-7968-42a3-d603-630b85671883"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.utils import np_utils\n",
        "from keras.layers.core import Dense, Activation, Dropout\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7XytccF5VME",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert list of labels to binary class matrix\n",
        "y_train_cat = np_utils.to_categorical(y_train) \n",
        "y_test_cat = np_utils.to_categorical(y_test) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pg6Zi4b35lWy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "b5577169-a75d-41ef-f1bc-5695d22f9e77"
      },
      "source": [
        "print(y_train[0:10])\n",
        "print(y_train_cat[0:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 1 1 1 1 1 0 0 1]\n",
            "[[1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNJkgEoj5odH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pre-processing: divide by max and substract mean\n",
        "scale = np.max(X_train)\n",
        "X_train_scale = X_train / scale\n",
        "X_test_scale = X_test / scale"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pKjC-qb52OP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "dde9fe3a-b25f-4474-b369-bbb2779843f5"
      },
      "source": [
        "print(X_train[0])\n",
        "print(X_train_scale[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.422e+01 2.312e+01 9.437e+01 6.099e+02 1.075e-01 2.413e-01 1.981e-01\n",
            " 6.618e-02 2.384e-01 7.542e-02 2.860e-01 2.110e+00 2.112e+00 3.172e+01\n",
            " 7.970e-03 1.354e-01 1.166e-01 1.666e-02 5.113e-02 1.172e-02 1.574e+01\n",
            " 3.718e+01 1.064e+02 7.624e+02 1.533e-01 9.327e-01 8.488e-01 1.772e-01\n",
            " 5.166e-01 1.446e-01]\n",
            "[3.34273625e-03 5.43488481e-03 2.21838270e-02 1.43370945e-01\n",
            " 2.52703338e-05 5.67230842e-05 4.65679361e-05 1.55571227e-05\n",
            " 5.60413728e-05 1.77291961e-05 6.72308416e-05 4.96003761e-04\n",
            " 4.96473907e-04 7.45651152e-03 1.87353079e-06 3.18288669e-05\n",
            " 2.74094969e-05 3.91631406e-06 1.20192760e-05 2.75505407e-06\n",
            " 3.70004701e-03 8.74000940e-03 2.50117536e-02 1.79219558e-01\n",
            " 3.60366714e-05 2.19252468e-04 1.99529854e-04 4.16549130e-05\n",
            " 1.21438646e-04 3.39915374e-05]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkJ3U9hn6CCE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "39933558-18a4-419d-bc27-454b72f8ec09"
      },
      "source": [
        "input_dim = X_train.shape[1]\n",
        "nb_classes = y_train.max()+1\n",
        "\n",
        "print('feature:', input_dim)\n",
        "print('class:', nb_classes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "feature: 30\n",
            "class: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2k7wJdb6Rsq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mlp_clf(input_dim, nb_classes):\n",
        "  model = Sequential()\n",
        "  model.add(Dense(128, input_dim=input_dim))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dropout(0.15))\n",
        "  model.add(Dense(128))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dropout(0.15))\n",
        "  model.add(Dense(nb_classes))\n",
        "  model.add(Activation('softmax'))\n",
        "\n",
        "  # we'll use categorical xent for the loss, and RMSprop as the optimizer\n",
        "  #model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
        "  model.compile(loss='binary_crossentropy', optimizer='rmsprop')\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEXQorBV6Zys",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b9fe059c-6b01-4fe2-e989-9eb7e6d5b052"
      },
      "source": [
        "mlp_model = mlp_clf(input_dim, nb_classes)\n",
        "\n",
        "print('Training...')\n",
        "mlp_model.fit(X_train_scale, y_train_cat, epochs=500, \n",
        "              batch_size=32, validation_split=0.1, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training...\n",
            "Train on 358 samples, validate on 40 samples\n",
            "Epoch 1/500\n",
            "358/358 [==============================] - 0s 411us/step - loss: 0.6599 - val_loss: 0.6070\n",
            "Epoch 2/500\n",
            "358/358 [==============================] - 0s 61us/step - loss: 0.5898 - val_loss: 0.5551\n",
            "Epoch 3/500\n",
            "358/358 [==============================] - 0s 56us/step - loss: 0.5232 - val_loss: 0.4812\n",
            "Epoch 4/500\n",
            "358/358 [==============================] - 0s 57us/step - loss: 0.4580 - val_loss: 0.3774\n",
            "Epoch 5/500\n",
            "358/358 [==============================] - 0s 57us/step - loss: 0.4019 - val_loss: 0.3289\n",
            "Epoch 6/500\n",
            "358/358 [==============================] - 0s 53us/step - loss: 0.3525 - val_loss: 0.3613\n",
            "Epoch 7/500\n",
            "358/358 [==============================] - 0s 54us/step - loss: 0.3335 - val_loss: 0.3024\n",
            "Epoch 8/500\n",
            "358/358 [==============================] - 0s 56us/step - loss: 0.3051 - val_loss: 0.2401\n",
            "Epoch 9/500\n",
            "358/358 [==============================] - 0s 69us/step - loss: 0.3171 - val_loss: 0.3208\n",
            "Epoch 10/500\n",
            "358/358 [==============================] - 0s 84us/step - loss: 0.2854 - val_loss: 0.2725\n",
            "Epoch 11/500\n",
            "358/358 [==============================] - 0s 69us/step - loss: 0.2762 - val_loss: 0.2188\n",
            "Epoch 12/500\n",
            "358/358 [==============================] - 0s 80us/step - loss: 0.2694 - val_loss: 0.2202\n",
            "Epoch 13/500\n",
            "358/358 [==============================] - 0s 88us/step - loss: 0.2681 - val_loss: 0.2384\n",
            "Epoch 14/500\n",
            "358/358 [==============================] - 0s 82us/step - loss: 0.2640 - val_loss: 0.2006\n",
            "Epoch 15/500\n",
            "358/358 [==============================] - 0s 88us/step - loss: 0.2667 - val_loss: 0.2368\n",
            "Epoch 16/500\n",
            "358/358 [==============================] - 0s 78us/step - loss: 0.2508 - val_loss: 0.1972\n",
            "Epoch 17/500\n",
            "358/358 [==============================] - 0s 77us/step - loss: 0.2542 - val_loss: 0.2534\n",
            "Epoch 18/500\n",
            "358/358 [==============================] - 0s 79us/step - loss: 0.2613 - val_loss: 0.2111\n",
            "Epoch 19/500\n",
            "358/358 [==============================] - 0s 80us/step - loss: 0.2494 - val_loss: 0.2309\n",
            "Epoch 20/500\n",
            "358/358 [==============================] - 0s 77us/step - loss: 0.2563 - val_loss: 0.2528\n",
            "Epoch 21/500\n",
            "358/358 [==============================] - 0s 76us/step - loss: 0.2467 - val_loss: 0.1906\n",
            "Epoch 22/500\n",
            "358/358 [==============================] - 0s 88us/step - loss: 0.2498 - val_loss: 0.2163\n",
            "Epoch 23/500\n",
            "358/358 [==============================] - 0s 84us/step - loss: 0.2601 - val_loss: 0.1939\n",
            "Epoch 24/500\n",
            "358/358 [==============================] - 0s 80us/step - loss: 0.2637 - val_loss: 0.1922\n",
            "Epoch 25/500\n",
            "358/358 [==============================] - 0s 90us/step - loss: 0.2537 - val_loss: 0.2889\n",
            "Epoch 26/500\n",
            "358/358 [==============================] - 0s 78us/step - loss: 0.2435 - val_loss: 0.1869\n",
            "Epoch 27/500\n",
            "358/358 [==============================] - 0s 93us/step - loss: 0.2471 - val_loss: 0.1956\n",
            "Epoch 28/500\n",
            "358/358 [==============================] - 0s 88us/step - loss: 0.2455 - val_loss: 0.3796\n",
            "Epoch 29/500\n",
            "358/358 [==============================] - 0s 75us/step - loss: 0.2531 - val_loss: 0.2137\n",
            "Epoch 30/500\n",
            "358/358 [==============================] - 0s 76us/step - loss: 0.2473 - val_loss: 0.2763\n",
            "Epoch 31/500\n",
            "358/358 [==============================] - 0s 57us/step - loss: 0.2509 - val_loss: 0.1986\n",
            "Epoch 32/500\n",
            "358/358 [==============================] - 0s 56us/step - loss: 0.2407 - val_loss: 0.3583\n",
            "Epoch 33/500\n",
            "358/358 [==============================] - 0s 59us/step - loss: 0.2471 - val_loss: 0.4120\n",
            "Epoch 34/500\n",
            "358/358 [==============================] - 0s 61us/step - loss: 0.2421 - val_loss: 0.2798\n",
            "Epoch 35/500\n",
            "358/358 [==============================] - 0s 60us/step - loss: 0.2541 - val_loss: 0.2053\n",
            "Epoch 36/500\n",
            "358/358 [==============================] - 0s 56us/step - loss: 0.2381 - val_loss: 0.1878\n",
            "Epoch 37/500\n",
            "358/358 [==============================] - 0s 57us/step - loss: 0.2517 - val_loss: 0.3587\n",
            "Epoch 38/500\n",
            "358/358 [==============================] - 0s 58us/step - loss: 0.2460 - val_loss: 0.1870\n",
            "Epoch 39/500\n",
            "358/358 [==============================] - 0s 63us/step - loss: 0.2441 - val_loss: 0.2082\n",
            "Epoch 40/500\n",
            "358/358 [==============================] - 0s 60us/step - loss: 0.2389 - val_loss: 0.2324\n",
            "Epoch 41/500\n",
            "358/358 [==============================] - 0s 63us/step - loss: 0.2426 - val_loss: 0.1773\n",
            "Epoch 42/500\n",
            "358/358 [==============================] - 0s 62us/step - loss: 0.2564 - val_loss: 0.2149\n",
            "Epoch 43/500\n",
            "358/358 [==============================] - 0s 56us/step - loss: 0.2367 - val_loss: 0.1856\n",
            "Epoch 44/500\n",
            "358/358 [==============================] - 0s 61us/step - loss: 0.2388 - val_loss: 0.1996\n",
            "Epoch 45/500\n",
            "358/358 [==============================] - 0s 60us/step - loss: 0.2409 - val_loss: 0.2098\n",
            "Epoch 46/500\n",
            "358/358 [==============================] - 0s 56us/step - loss: 0.2361 - val_loss: 0.2419\n",
            "Epoch 47/500\n",
            "358/358 [==============================] - 0s 62us/step - loss: 0.2358 - val_loss: 0.1940\n",
            "Epoch 48/500\n",
            "358/358 [==============================] - 0s 66us/step - loss: 0.2392 - val_loss: 0.2462\n",
            "Epoch 49/500\n",
            "358/358 [==============================] - 0s 60us/step - loss: 0.2364 - val_loss: 0.2521\n",
            "Epoch 50/500\n",
            "358/358 [==============================] - 0s 60us/step - loss: 0.2352 - val_loss: 0.2354\n",
            "Epoch 51/500\n",
            "358/358 [==============================] - 0s 90us/step - loss: 0.2289 - val_loss: 0.1688\n",
            "Epoch 52/500\n",
            "358/358 [==============================] - 0s 95us/step - loss: 0.2388 - val_loss: 0.2758\n",
            "Epoch 53/500\n",
            "358/358 [==============================] - 0s 86us/step - loss: 0.2379 - val_loss: 0.1859\n",
            "Epoch 54/500\n",
            "358/358 [==============================] - 0s 87us/step - loss: 0.2427 - val_loss: 0.1905\n",
            "Epoch 55/500\n",
            "358/358 [==============================] - 0s 85us/step - loss: 0.2254 - val_loss: 0.1639\n",
            "Epoch 56/500\n",
            "358/358 [==============================] - 0s 82us/step - loss: 0.2450 - val_loss: 0.2458\n",
            "Epoch 57/500\n",
            "358/358 [==============================] - 0s 84us/step - loss: 0.2216 - val_loss: 0.2705\n",
            "Epoch 58/500\n",
            "358/358 [==============================] - 0s 81us/step - loss: 0.2369 - val_loss: 0.1866\n",
            "Epoch 59/500\n",
            "358/358 [==============================] - 0s 84us/step - loss: 0.2471 - val_loss: 0.1691\n",
            "Epoch 60/500\n",
            "358/358 [==============================] - 0s 88us/step - loss: 0.2329 - val_loss: 0.1627\n",
            "Epoch 61/500\n",
            "358/358 [==============================] - 0s 83us/step - loss: 0.2325 - val_loss: 0.2583\n",
            "Epoch 62/500\n",
            "358/358 [==============================] - 0s 86us/step - loss: 0.2314 - val_loss: 0.2747\n",
            "Epoch 63/500\n",
            "358/358 [==============================] - 0s 84us/step - loss: 0.2219 - val_loss: 0.1737\n",
            "Epoch 64/500\n",
            "358/358 [==============================] - 0s 82us/step - loss: 0.2449 - val_loss: 0.1947\n",
            "Epoch 65/500\n",
            "358/358 [==============================] - 0s 84us/step - loss: 0.2405 - val_loss: 0.1864\n",
            "Epoch 66/500\n",
            "358/358 [==============================] - 0s 82us/step - loss: 0.2251 - val_loss: 0.2125\n",
            "Epoch 67/500\n",
            "358/358 [==============================] - 0s 81us/step - loss: 0.2279 - val_loss: 0.2346\n",
            "Epoch 68/500\n",
            "358/358 [==============================] - 0s 68us/step - loss: 0.2284 - val_loss: 0.1823\n",
            "Epoch 69/500\n",
            "358/358 [==============================] - 0s 62us/step - loss: 0.2344 - val_loss: 0.1725\n",
            "Epoch 70/500\n",
            "358/358 [==============================] - 0s 58us/step - loss: 0.2280 - val_loss: 0.1849\n",
            "Epoch 71/500\n",
            "358/358 [==============================] - 0s 55us/step - loss: 0.2283 - val_loss: 0.1914\n",
            "Epoch 72/500\n",
            "358/358 [==============================] - 0s 54us/step - loss: 0.2358 - val_loss: 0.2725\n",
            "Epoch 73/500\n",
            "358/358 [==============================] - 0s 56us/step - loss: 0.2167 - val_loss: 0.1735\n",
            "Epoch 74/500\n",
            "358/358 [==============================] - 0s 55us/step - loss: 0.2247 - val_loss: 0.1905\n",
            "Epoch 75/500\n",
            "358/358 [==============================] - 0s 55us/step - loss: 0.2284 - val_loss: 0.1906\n",
            "Epoch 76/500\n",
            "358/358 [==============================] - 0s 55us/step - loss: 0.2164 - val_loss: 0.1595\n",
            "Epoch 77/500\n",
            "358/358 [==============================] - 0s 58us/step - loss: 0.2153 - val_loss: 0.2263\n",
            "Epoch 78/500\n",
            "358/358 [==============================] - 0s 54us/step - loss: 0.2262 - val_loss: 0.1935\n",
            "Epoch 79/500\n",
            "358/358 [==============================] - 0s 60us/step - loss: 0.2262 - val_loss: 0.2255\n",
            "Epoch 80/500\n",
            "358/358 [==============================] - 0s 57us/step - loss: 0.2140 - val_loss: 0.2838\n",
            "Epoch 81/500\n",
            "358/358 [==============================] - 0s 54us/step - loss: 0.2213 - val_loss: 0.2901\n",
            "Epoch 82/500\n",
            "358/358 [==============================] - 0s 56us/step - loss: 0.2266 - val_loss: 0.2448\n",
            "Epoch 83/500\n",
            "358/358 [==============================] - 0s 69us/step - loss: 0.2163 - val_loss: 0.1666\n",
            "Epoch 84/500\n",
            "358/358 [==============================] - 0s 67us/step - loss: 0.2214 - val_loss: 0.1429\n",
            "Epoch 85/500\n",
            "358/358 [==============================] - 0s 86us/step - loss: 0.2186 - val_loss: 0.1682\n",
            "Epoch 86/500\n",
            "358/358 [==============================] - 0s 88us/step - loss: 0.2206 - val_loss: 0.1750\n",
            "Epoch 87/500\n",
            "358/358 [==============================] - 0s 90us/step - loss: 0.2286 - val_loss: 0.2028\n",
            "Epoch 88/500\n",
            "358/358 [==============================] - 0s 84us/step - loss: 0.2129 - val_loss: 0.1775\n",
            "Epoch 89/500\n",
            "358/358 [==============================] - 0s 87us/step - loss: 0.2213 - val_loss: 0.2118\n",
            "Epoch 90/500\n",
            "358/358 [==============================] - 0s 76us/step - loss: 0.2072 - val_loss: 0.1800\n",
            "Epoch 91/500\n",
            "358/358 [==============================] - 0s 83us/step - loss: 0.2175 - val_loss: 0.1499\n",
            "Epoch 92/500\n",
            "358/358 [==============================] - 0s 83us/step - loss: 0.2175 - val_loss: 0.2444\n",
            "Epoch 93/500\n",
            "358/358 [==============================] - 0s 75us/step - loss: 0.2139 - val_loss: 0.1819\n",
            "Epoch 94/500\n",
            "358/358 [==============================] - 0s 80us/step - loss: 0.2112 - val_loss: 0.2572\n",
            "Epoch 95/500\n",
            "358/358 [==============================] - 0s 82us/step - loss: 0.2107 - val_loss: 0.1754\n",
            "Epoch 96/500\n",
            "358/358 [==============================] - 0s 86us/step - loss: 0.2030 - val_loss: 0.1454\n",
            "Epoch 97/500\n",
            "358/358 [==============================] - 0s 79us/step - loss: 0.2112 - val_loss: 0.1955\n",
            "Epoch 98/500\n",
            "358/358 [==============================] - 0s 82us/step - loss: 0.2097 - val_loss: 0.2869\n",
            "Epoch 99/500\n",
            "358/358 [==============================] - 0s 82us/step - loss: 0.2211 - val_loss: 0.1553\n",
            "Epoch 100/500\n",
            "358/358 [==============================] - 0s 76us/step - loss: 0.2147 - val_loss: 0.1884\n",
            "Epoch 101/500\n",
            "358/358 [==============================] - 0s 80us/step - loss: 0.2192 - val_loss: 0.1332\n",
            "Epoch 102/500\n",
            "358/358 [==============================] - 0s 91us/step - loss: 0.2060 - val_loss: 0.1411\n",
            "Epoch 103/500\n",
            "358/358 [==============================] - 0s 76us/step - loss: 0.2096 - val_loss: 0.1789\n",
            "Epoch 104/500\n",
            "358/358 [==============================] - 0s 77us/step - loss: 0.2019 - val_loss: 0.1811\n",
            "Epoch 105/500\n",
            "358/358 [==============================] - 0s 80us/step - loss: 0.2151 - val_loss: 0.1372\n",
            "Epoch 106/500\n",
            "358/358 [==============================] - 0s 60us/step - loss: 0.2172 - val_loss: 0.1545\n",
            "Epoch 107/500\n",
            "358/358 [==============================] - 0s 67us/step - loss: 0.2107 - val_loss: 0.2288\n",
            "Epoch 108/500\n",
            "358/358 [==============================] - 0s 63us/step - loss: 0.2068 - val_loss: 0.2723\n",
            "Epoch 109/500\n",
            "358/358 [==============================] - 0s 59us/step - loss: 0.1977 - val_loss: 0.1225\n",
            "Epoch 110/500\n",
            "358/358 [==============================] - 0s 56us/step - loss: 0.2095 - val_loss: 0.2793\n",
            "Epoch 111/500\n",
            "358/358 [==============================] - 0s 55us/step - loss: 0.2044 - val_loss: 0.2014\n",
            "Epoch 112/500\n",
            "358/358 [==============================] - 0s 56us/step - loss: 0.2106 - val_loss: 0.2463\n",
            "Epoch 113/500\n",
            "358/358 [==============================] - 0s 60us/step - loss: 0.2151 - val_loss: 0.2816\n",
            "Epoch 114/500\n",
            "358/358 [==============================] - 0s 56us/step - loss: 0.2192 - val_loss: 0.1571\n",
            "Epoch 115/500\n",
            "358/358 [==============================] - 0s 58us/step - loss: 0.1963 - val_loss: 0.3434\n",
            "Epoch 116/500\n",
            "358/358 [==============================] - 0s 59us/step - loss: 0.2134 - val_loss: 0.1285\n",
            "Epoch 117/500\n",
            "358/358 [==============================] - 0s 57us/step - loss: 0.2028 - val_loss: 0.3104\n",
            "Epoch 118/500\n",
            "358/358 [==============================] - 0s 59us/step - loss: 0.2045 - val_loss: 0.2005\n",
            "Epoch 119/500\n",
            "358/358 [==============================] - 0s 62us/step - loss: 0.2008 - val_loss: 0.1408\n",
            "Epoch 120/500\n",
            "358/358 [==============================] - 0s 82us/step - loss: 0.2041 - val_loss: 0.1381\n",
            "Epoch 121/500\n",
            "358/358 [==============================] - 0s 86us/step - loss: 0.2125 - val_loss: 0.1786\n",
            "Epoch 122/500\n",
            "358/358 [==============================] - 0s 91us/step - loss: 0.1973 - val_loss: 0.1133\n",
            "Epoch 123/500\n",
            "358/358 [==============================] - 0s 100us/step - loss: 0.2016 - val_loss: 0.1288\n",
            "Epoch 124/500\n",
            "358/358 [==============================] - 0s 89us/step - loss: 0.2143 - val_loss: 0.1564\n",
            "Epoch 125/500\n",
            "358/358 [==============================] - 0s 78us/step - loss: 0.2035 - val_loss: 0.1237\n",
            "Epoch 126/500\n",
            "358/358 [==============================] - 0s 83us/step - loss: 0.2052 - val_loss: 0.1144\n",
            "Epoch 127/500\n",
            "358/358 [==============================] - 0s 86us/step - loss: 0.2044 - val_loss: 0.1391\n",
            "Epoch 128/500\n",
            "358/358 [==============================] - 0s 80us/step - loss: 0.2030 - val_loss: 0.1848\n",
            "Epoch 129/500\n",
            "358/358 [==============================] - 0s 85us/step - loss: 0.1893 - val_loss: 0.1071\n",
            "Epoch 130/500\n",
            "358/358 [==============================] - 0s 85us/step - loss: 0.2121 - val_loss: 0.1797\n",
            "Epoch 131/500\n",
            "358/358 [==============================] - 0s 79us/step - loss: 0.1940 - val_loss: 0.1312\n",
            "Epoch 132/500\n",
            "358/358 [==============================] - 0s 81us/step - loss: 0.2097 - val_loss: 0.1176\n",
            "Epoch 133/500\n",
            "358/358 [==============================] - 0s 83us/step - loss: 0.1989 - val_loss: 0.1321\n",
            "Epoch 134/500\n",
            "358/358 [==============================] - 0s 80us/step - loss: 0.2063 - val_loss: 0.1129\n",
            "Epoch 135/500\n",
            "358/358 [==============================] - 0s 85us/step - loss: 0.2026 - val_loss: 0.1390\n",
            "Epoch 136/500\n",
            "358/358 [==============================] - 0s 90us/step - loss: 0.1986 - val_loss: 0.1857\n",
            "Epoch 137/500\n",
            "358/358 [==============================] - 0s 61us/step - loss: 0.1991 - val_loss: 0.2130\n",
            "Epoch 138/500\n",
            "358/358 [==============================] - 0s 57us/step - loss: 0.2090 - val_loss: 0.1121\n",
            "Epoch 139/500\n",
            "358/358 [==============================] - 0s 62us/step - loss: 0.2036 - val_loss: 0.2146\n",
            "Epoch 140/500\n",
            "358/358 [==============================] - 0s 57us/step - loss: 0.1981 - val_loss: 0.1637\n",
            "Epoch 141/500\n",
            "358/358 [==============================] - 0s 57us/step - loss: 0.1972 - val_loss: 0.1118\n",
            "Epoch 142/500\n",
            "358/358 [==============================] - 0s 56us/step - loss: 0.2057 - val_loss: 0.2048\n",
            "Epoch 143/500\n",
            "358/358 [==============================] - 0s 67us/step - loss: 0.2043 - val_loss: 0.2029\n",
            "Epoch 144/500\n",
            "358/358 [==============================] - 0s 85us/step - loss: 0.1999 - val_loss: 0.1291\n",
            "Epoch 145/500\n",
            "358/358 [==============================] - 0s 88us/step - loss: 0.1906 - val_loss: 0.3722\n",
            "Epoch 146/500\n",
            "358/358 [==============================] - 0s 84us/step - loss: 0.2057 - val_loss: 0.3440\n",
            "Epoch 147/500\n",
            "358/358 [==============================] - 0s 84us/step - loss: 0.1980 - val_loss: 0.3514\n",
            "Epoch 148/500\n",
            "358/358 [==============================] - 0s 79us/step - loss: 0.1913 - val_loss: 0.1878\n",
            "Epoch 149/500\n",
            "358/358 [==============================] - 0s 59us/step - loss: 0.2023 - val_loss: 0.1402\n",
            "Epoch 150/500\n",
            "358/358 [==============================] - 0s 64us/step - loss: 0.2044 - val_loss: 0.4263\n",
            "Epoch 151/500\n",
            "358/358 [==============================] - 0s 61us/step - loss: 0.2049 - val_loss: 0.1767\n",
            "Epoch 152/500\n",
            "358/358 [==============================] - 0s 54us/step - loss: 0.1885 - val_loss: 0.0962\n",
            "Epoch 153/500\n",
            "358/358 [==============================] - 0s 56us/step - loss: 0.1897 - val_loss: 0.1119\n",
            "Epoch 154/500\n",
            "358/358 [==============================] - 0s 61us/step - loss: 0.1958 - val_loss: 0.3154\n",
            "Epoch 155/500\n",
            "358/358 [==============================] - 0s 57us/step - loss: 0.1946 - val_loss: 0.1563\n",
            "Epoch 156/500\n",
            "358/358 [==============================] - 0s 56us/step - loss: 0.1872 - val_loss: 0.1466\n",
            "Epoch 157/500\n",
            "358/358 [==============================] - 0s 55us/step - loss: 0.1963 - val_loss: 0.1972\n",
            "Epoch 158/500\n",
            "358/358 [==============================] - 0s 54us/step - loss: 0.1923 - val_loss: 0.2670\n",
            "Epoch 159/500\n",
            "358/358 [==============================] - 0s 56us/step - loss: 0.1898 - val_loss: 0.2546\n",
            "Epoch 160/500\n",
            "358/358 [==============================] - 0s 61us/step - loss: 0.1885 - val_loss: 0.2454\n",
            "Epoch 161/500\n",
            "358/358 [==============================] - 0s 109us/step - loss: 0.1921 - val_loss: 0.1034\n",
            "Epoch 162/500\n",
            "358/358 [==============================] - 0s 102us/step - loss: 0.1818 - val_loss: 0.1669\n",
            "Epoch 163/500\n",
            "358/358 [==============================] - 0s 93us/step - loss: 0.1987 - val_loss: 0.1713\n",
            "Epoch 164/500\n",
            "358/358 [==============================] - 0s 98us/step - loss: 0.1857 - val_loss: 0.0855\n",
            "Epoch 165/500\n",
            "358/358 [==============================] - 0s 86us/step - loss: 0.1885 - val_loss: 0.2359\n",
            "Epoch 166/500\n",
            "358/358 [==============================] - 0s 88us/step - loss: 0.1797 - val_loss: 0.1132\n",
            "Epoch 167/500\n",
            "358/358 [==============================] - 0s 74us/step - loss: 0.1877 - val_loss: 0.1985\n",
            "Epoch 168/500\n",
            "358/358 [==============================] - 0s 76us/step - loss: 0.1885 - val_loss: 0.0840\n",
            "Epoch 169/500\n",
            "358/358 [==============================] - 0s 79us/step - loss: 0.1871 - val_loss: 0.1488\n",
            "Epoch 170/500\n",
            "358/358 [==============================] - 0s 81us/step - loss: 0.1888 - val_loss: 0.0881\n",
            "Epoch 171/500\n",
            "358/358 [==============================] - 0s 73us/step - loss: 0.1881 - val_loss: 0.3182\n",
            "Epoch 172/500\n",
            "358/358 [==============================] - 0s 79us/step - loss: 0.1917 - val_loss: 0.1122\n",
            "Epoch 173/500\n",
            "358/358 [==============================] - 0s 74us/step - loss: 0.2039 - val_loss: 0.1466\n",
            "Epoch 174/500\n",
            "358/358 [==============================] - 0s 95us/step - loss: 0.1872 - val_loss: 0.3076\n",
            "Epoch 175/500\n",
            "358/358 [==============================] - 0s 86us/step - loss: 0.1804 - val_loss: 0.0822\n",
            "Epoch 176/500\n",
            "358/358 [==============================] - 0s 94us/step - loss: 0.2007 - val_loss: 0.1084\n",
            "Epoch 177/500\n",
            "358/358 [==============================] - 0s 85us/step - loss: 0.1861 - val_loss: 0.1062\n",
            "Epoch 178/500\n",
            "358/358 [==============================] - 0s 87us/step - loss: 0.1782 - val_loss: 0.3296\n",
            "Epoch 179/500\n",
            "358/358 [==============================] - 0s 69us/step - loss: 0.1921 - val_loss: 0.1154\n",
            "Epoch 180/500\n",
            "358/358 [==============================] - 0s 63us/step - loss: 0.1818 - val_loss: 0.1631\n",
            "Epoch 181/500\n",
            "358/358 [==============================] - 0s 61us/step - loss: 0.1801 - val_loss: 0.3351\n",
            "Epoch 182/500\n",
            "358/358 [==============================] - 0s 61us/step - loss: 0.1880 - val_loss: 0.3510\n",
            "Epoch 183/500\n",
            "358/358 [==============================] - 0s 62us/step - loss: 0.2019 - val_loss: 0.1476\n",
            "Epoch 184/500\n",
            "358/358 [==============================] - 0s 65us/step - loss: 0.1910 - val_loss: 0.2759\n",
            "Epoch 185/500\n",
            "358/358 [==============================] - 0s 64us/step - loss: 0.1940 - val_loss: 0.1771\n",
            "Epoch 186/500\n",
            "358/358 [==============================] - 0s 71us/step - loss: 0.1763 - val_loss: 0.0736\n",
            "Epoch 187/500\n",
            "358/358 [==============================] - 0s 64us/step - loss: 0.1959 - val_loss: 0.1578\n",
            "Epoch 188/500\n",
            "358/358 [==============================] - 0s 61us/step - loss: 0.1962 - val_loss: 0.3379\n",
            "Epoch 189/500\n",
            "358/358 [==============================] - 0s 62us/step - loss: 0.1912 - val_loss: 0.1028\n",
            "Epoch 190/500\n",
            "358/358 [==============================] - 0s 63us/step - loss: 0.1770 - val_loss: 0.0787\n",
            "Epoch 191/500\n",
            "358/358 [==============================] - 0s 68us/step - loss: 0.1851 - val_loss: 0.0797\n",
            "Epoch 192/500\n",
            "358/358 [==============================] - 0s 63us/step - loss: 0.1815 - val_loss: 0.1287\n",
            "Epoch 193/500\n",
            "358/358 [==============================] - 0s 65us/step - loss: 0.1750 - val_loss: 0.0827\n",
            "Epoch 194/500\n",
            "358/358 [==============================] - 0s 64us/step - loss: 0.1696 - val_loss: 0.1632\n",
            "Epoch 195/500\n",
            "358/358 [==============================] - 0s 62us/step - loss: 0.1904 - val_loss: 0.1302\n",
            "Epoch 196/500\n",
            "358/358 [==============================] - 0s 69us/step - loss: 0.1606 - val_loss: 0.1456\n",
            "Epoch 197/500\n",
            "358/358 [==============================] - 0s 64us/step - loss: 0.1955 - val_loss: 0.1420\n",
            "Epoch 198/500\n",
            "358/358 [==============================] - 0s 72us/step - loss: 0.1717 - val_loss: 0.2751\n",
            "Epoch 199/500\n",
            "358/358 [==============================] - 0s 62us/step - loss: 0.1783 - val_loss: 0.0841\n",
            "Epoch 200/500\n",
            "358/358 [==============================] - 0s 71us/step - loss: 0.1847 - val_loss: 0.2692\n",
            "Epoch 201/500\n",
            "358/358 [==============================] - 0s 70us/step - loss: 0.1655 - val_loss: 0.2112\n",
            "Epoch 202/500\n",
            "358/358 [==============================] - 0s 71us/step - loss: 0.1753 - val_loss: 0.0947\n",
            "Epoch 203/500\n",
            "358/358 [==============================] - 0s 64us/step - loss: 0.1772 - val_loss: 0.3048\n",
            "Epoch 204/500\n",
            "358/358 [==============================] - 0s 62us/step - loss: 0.1780 - val_loss: 0.2663\n",
            "Epoch 205/500\n",
            "358/358 [==============================] - 0s 65us/step - loss: 0.1768 - val_loss: 0.1662\n",
            "Epoch 206/500\n",
            "358/358 [==============================] - 0s 66us/step - loss: 0.1768 - val_loss: 0.2778\n",
            "Epoch 207/500\n",
            "358/358 [==============================] - 0s 59us/step - loss: 0.1838 - val_loss: 0.5811\n",
            "Epoch 208/500\n",
            "358/358 [==============================] - 0s 71us/step - loss: 0.1999 - val_loss: 0.2272\n",
            "Epoch 209/500\n",
            "358/358 [==============================] - 0s 62us/step - loss: 0.1745 - val_loss: 0.2714\n",
            "Epoch 210/500\n",
            "358/358 [==============================] - 0s 60us/step - loss: 0.1747 - val_loss: 0.3312\n",
            "Epoch 211/500\n",
            "358/358 [==============================] - 0s 59us/step - loss: 0.1781 - val_loss: 0.0844\n",
            "Epoch 212/500\n",
            "358/358 [==============================] - 0s 57us/step - loss: 0.1687 - val_loss: 0.2273\n",
            "Epoch 213/500\n",
            "358/358 [==============================] - 0s 64us/step - loss: 0.1777 - val_loss: 0.1130\n",
            "Epoch 214/500\n",
            "358/358 [==============================] - 0s 73us/step - loss: 0.1660 - val_loss: 0.0574\n",
            "Epoch 215/500\n",
            "358/358 [==============================] - 0s 64us/step - loss: 0.1870 - val_loss: 0.0768\n",
            "Epoch 216/500\n",
            "358/358 [==============================] - 0s 65us/step - loss: 0.1795 - val_loss: 0.2743\n",
            "Epoch 217/500\n",
            "358/358 [==============================] - 0s 68us/step - loss: 0.1614 - val_loss: 0.1305\n",
            "Epoch 218/500\n",
            "358/358 [==============================] - 0s 62us/step - loss: 0.1731 - val_loss: 0.4315\n",
            "Epoch 219/500\n",
            "358/358 [==============================] - 0s 77us/step - loss: 0.1908 - val_loss: 0.1614\n",
            "Epoch 220/500\n",
            "358/358 [==============================] - 0s 75us/step - loss: 0.1791 - val_loss: 0.0959\n",
            "Epoch 221/500\n",
            "358/358 [==============================] - 0s 73us/step - loss: 0.1763 - val_loss: 0.0655\n",
            "Epoch 222/500\n",
            "358/358 [==============================] - 0s 74us/step - loss: 0.1670 - val_loss: 0.3878\n",
            "Epoch 223/500\n",
            "358/358 [==============================] - 0s 70us/step - loss: 0.1699 - val_loss: 0.4758\n",
            "Epoch 224/500\n",
            "358/358 [==============================] - 0s 76us/step - loss: 0.1841 - val_loss: 0.0842\n",
            "Epoch 225/500\n",
            "358/358 [==============================] - 0s 71us/step - loss: 0.1843 - val_loss: 0.0895\n",
            "Epoch 226/500\n",
            "358/358 [==============================] - 0s 76us/step - loss: 0.1654 - val_loss: 0.1413\n",
            "Epoch 227/500\n",
            "358/358 [==============================] - 0s 73us/step - loss: 0.1763 - val_loss: 0.1474\n",
            "Epoch 228/500\n",
            "358/358 [==============================] - 0s 73us/step - loss: 0.1631 - val_loss: 0.2819\n",
            "Epoch 229/500\n",
            "358/358 [==============================] - 0s 74us/step - loss: 0.1701 - val_loss: 0.1047\n",
            "Epoch 230/500\n",
            "358/358 [==============================] - 0s 69us/step - loss: 0.1731 - val_loss: 0.0781\n",
            "Epoch 231/500\n",
            "358/358 [==============================] - 0s 72us/step - loss: 0.1672 - val_loss: 0.0864\n",
            "Epoch 232/500\n",
            "358/358 [==============================] - 0s 71us/step - loss: 0.1826 - val_loss: 0.3828\n",
            "Epoch 233/500\n",
            "358/358 [==============================] - 0s 82us/step - loss: 0.1775 - val_loss: 0.0772\n",
            "Epoch 234/500\n",
            "358/358 [==============================] - 0s 74us/step - loss: 0.1694 - val_loss: 0.1022\n",
            "Epoch 235/500\n",
            "358/358 [==============================] - 0s 79us/step - loss: 0.1817 - val_loss: 0.1812\n",
            "Epoch 236/500\n",
            "358/358 [==============================] - 0s 78us/step - loss: 0.1601 - val_loss: 0.0559\n",
            "Epoch 237/500\n",
            "358/358 [==============================] - 0s 80us/step - loss: 0.2001 - val_loss: 0.4060\n",
            "Epoch 238/500\n",
            "358/358 [==============================] - 0s 87us/step - loss: 0.1785 - val_loss: 0.3100\n",
            "Epoch 239/500\n",
            "358/358 [==============================] - 0s 79us/step - loss: 0.1699 - val_loss: 0.2554\n",
            "Epoch 240/500\n",
            "358/358 [==============================] - 0s 71us/step - loss: 0.1646 - val_loss: 0.1254\n",
            "Epoch 241/500\n",
            "358/358 [==============================] - 0s 74us/step - loss: 0.1635 - val_loss: 0.1641\n",
            "Epoch 242/500\n",
            "358/358 [==============================] - 0s 78us/step - loss: 0.1657 - val_loss: 0.4822\n",
            "Epoch 243/500\n",
            "358/358 [==============================] - 0s 75us/step - loss: 0.1756 - val_loss: 0.3427\n",
            "Epoch 244/500\n",
            "358/358 [==============================] - 0s 73us/step - loss: 0.1751 - val_loss: 0.0804\n",
            "Epoch 245/500\n",
            "358/358 [==============================] - 0s 79us/step - loss: 0.1677 - val_loss: 0.3846\n",
            "Epoch 246/500\n",
            "358/358 [==============================] - 0s 71us/step - loss: 0.1860 - val_loss: 0.3831\n",
            "Epoch 247/500\n",
            "358/358 [==============================] - 0s 75us/step - loss: 0.1737 - val_loss: 0.2386\n",
            "Epoch 248/500\n",
            "358/358 [==============================] - 0s 71us/step - loss: 0.1592 - val_loss: 0.1528\n",
            "Epoch 249/500\n",
            "358/358 [==============================] - 0s 88us/step - loss: 0.1774 - val_loss: 0.2039\n",
            "Epoch 250/500\n",
            "358/358 [==============================] - 0s 78us/step - loss: 0.1662 - val_loss: 0.2196\n",
            "Epoch 251/500\n",
            "358/358 [==============================] - 0s 74us/step - loss: 0.1677 - val_loss: 0.0720\n",
            "Epoch 252/500\n",
            "358/358 [==============================] - 0s 72us/step - loss: 0.1788 - val_loss: 0.2034\n",
            "Epoch 253/500\n",
            "358/358 [==============================] - 0s 70us/step - loss: 0.1730 - val_loss: 0.1224\n",
            "Epoch 254/500\n",
            "358/358 [==============================] - 0s 69us/step - loss: 0.1607 - val_loss: 0.2821\n",
            "Epoch 255/500\n",
            "358/358 [==============================] - 0s 73us/step - loss: 0.1690 - val_loss: 0.1285\n",
            "Epoch 256/500\n",
            "358/358 [==============================] - 0s 75us/step - loss: 0.1626 - val_loss: 0.0955\n",
            "Epoch 257/500\n",
            "358/358 [==============================] - 0s 69us/step - loss: 0.1685 - val_loss: 0.1366\n",
            "Epoch 258/500\n",
            "358/358 [==============================] - 0s 72us/step - loss: 0.1654 - val_loss: 0.1542\n",
            "Epoch 259/500\n",
            "358/358 [==============================] - 0s 72us/step - loss: 0.1631 - val_loss: 0.0994\n",
            "Epoch 260/500\n",
            "358/358 [==============================] - 0s 75us/step - loss: 0.1639 - val_loss: 0.0566\n",
            "Epoch 261/500\n",
            "358/358 [==============================] - 0s 74us/step - loss: 0.1593 - val_loss: 0.2968\n",
            "Epoch 262/500\n",
            "358/358 [==============================] - 0s 72us/step - loss: 0.1716 - val_loss: 0.1112\n",
            "Epoch 263/500\n",
            "358/358 [==============================] - 0s 76us/step - loss: 0.1789 - val_loss: 0.0969\n",
            "Epoch 264/500\n",
            "358/358 [==============================] - 0s 70us/step - loss: 0.1627 - val_loss: 0.1053\n",
            "Epoch 265/500\n",
            "358/358 [==============================] - 0s 73us/step - loss: 0.1671 - val_loss: 0.0876\n",
            "Epoch 266/500\n",
            "358/358 [==============================] - 0s 73us/step - loss: 0.1605 - val_loss: 0.1594\n",
            "Epoch 267/500\n",
            "358/358 [==============================] - 0s 72us/step - loss: 0.1552 - val_loss: 0.0951\n",
            "Epoch 268/500\n",
            "358/358 [==============================] - 0s 73us/step - loss: 0.1699 - val_loss: 0.0517\n",
            "Epoch 269/500\n",
            "358/358 [==============================] - 0s 72us/step - loss: 0.1519 - val_loss: 0.0950\n",
            "Epoch 270/500\n",
            "358/358 [==============================] - 0s 75us/step - loss: 0.1822 - val_loss: 0.1814\n",
            "Epoch 271/500\n",
            "358/358 [==============================] - 0s 71us/step - loss: 0.1582 - val_loss: 0.0994\n",
            "Epoch 272/500\n",
            "358/358 [==============================] - 0s 73us/step - loss: 0.1583 - val_loss: 0.1819\n",
            "Epoch 273/500\n",
            "358/358 [==============================] - 0s 74us/step - loss: 0.1906 - val_loss: 0.0583\n",
            "Epoch 274/500\n",
            "358/358 [==============================] - 0s 64us/step - loss: 0.1718 - val_loss: 0.1032\n",
            "Epoch 275/500\n",
            "358/358 [==============================] - 0s 64us/step - loss: 0.1691 - val_loss: 0.1139\n",
            "Epoch 276/500\n",
            "358/358 [==============================] - 0s 60us/step - loss: 0.1682 - val_loss: 0.0652\n",
            "Epoch 277/500\n",
            "358/358 [==============================] - 0s 57us/step - loss: 0.1873 - val_loss: 0.0600\n",
            "Epoch 278/500\n",
            "358/358 [==============================] - 0s 59us/step - loss: 0.1689 - val_loss: 0.1582\n",
            "Epoch 279/500\n",
            "358/358 [==============================] - 0s 66us/step - loss: 0.1591 - val_loss: 0.3026\n",
            "Epoch 280/500\n",
            "358/358 [==============================] - 0s 63us/step - loss: 0.1692 - val_loss: 0.0796\n",
            "Epoch 281/500\n",
            "358/358 [==============================] - 0s 57us/step - loss: 0.1755 - val_loss: 0.1591\n",
            "Epoch 282/500\n",
            "358/358 [==============================] - 0s 58us/step - loss: 0.1537 - val_loss: 0.0985\n",
            "Epoch 283/500\n",
            "358/358 [==============================] - 0s 56us/step - loss: 0.1496 - val_loss: 0.1635\n",
            "Epoch 284/500\n",
            "358/358 [==============================] - 0s 62us/step - loss: 0.1595 - val_loss: 0.4302\n",
            "Epoch 285/500\n",
            "358/358 [==============================] - 0s 59us/step - loss: 0.1700 - val_loss: 0.1261\n",
            "Epoch 286/500\n",
            "358/358 [==============================] - 0s 59us/step - loss: 0.1654 - val_loss: 0.3433\n",
            "Epoch 287/500\n",
            "358/358 [==============================] - 0s 59us/step - loss: 0.1702 - val_loss: 0.2522\n",
            "Epoch 288/500\n",
            "358/358 [==============================] - 0s 59us/step - loss: 0.1585 - val_loss: 0.2809\n",
            "Epoch 289/500\n",
            "358/358 [==============================] - 0s 61us/step - loss: 0.1549 - val_loss: 0.0634\n",
            "Epoch 290/500\n",
            "358/358 [==============================] - 0s 59us/step - loss: 0.1613 - val_loss: 0.0935\n",
            "Epoch 291/500\n",
            "358/358 [==============================] - 0s 62us/step - loss: 0.1592 - val_loss: 0.0683\n",
            "Epoch 292/500\n",
            "358/358 [==============================] - 0s 96us/step - loss: 0.1592 - val_loss: 0.1481\n",
            "Epoch 293/500\n",
            "358/358 [==============================] - 0s 84us/step - loss: 0.1714 - val_loss: 0.2035\n",
            "Epoch 294/500\n",
            "358/358 [==============================] - 0s 75us/step - loss: 0.1704 - val_loss: 0.0952\n",
            "Epoch 295/500\n",
            "358/358 [==============================] - 0s 61us/step - loss: 0.1662 - val_loss: 0.1157\n",
            "Epoch 296/500\n",
            "358/358 [==============================] - 0s 61us/step - loss: 0.1542 - val_loss: 0.3343\n",
            "Epoch 297/500\n",
            "358/358 [==============================] - 0s 58us/step - loss: 0.1633 - val_loss: 0.0566\n",
            "Epoch 298/500\n",
            "358/358 [==============================] - 0s 59us/step - loss: 0.1531 - val_loss: 0.3509\n",
            "Epoch 299/500\n",
            "358/358 [==============================] - 0s 57us/step - loss: 0.1827 - val_loss: 0.2568\n",
            "Epoch 300/500\n",
            "358/358 [==============================] - 0s 62us/step - loss: 0.1614 - val_loss: 0.2346\n",
            "Epoch 301/500\n",
            "358/358 [==============================] - 0s 57us/step - loss: 0.1563 - val_loss: 0.0567\n",
            "Epoch 302/500\n",
            "358/358 [==============================] - 0s 54us/step - loss: 0.1751 - val_loss: 0.0981\n",
            "Epoch 303/500\n",
            "358/358 [==============================] - 0s 65us/step - loss: 0.1606 - val_loss: 0.1705\n",
            "Epoch 304/500\n",
            "358/358 [==============================] - 0s 61us/step - loss: 0.1551 - val_loss: 0.1033\n",
            "Epoch 305/500\n",
            "358/358 [==============================] - 0s 62us/step - loss: 0.1478 - val_loss: 0.0396\n",
            "Epoch 306/500\n",
            "358/358 [==============================] - 0s 73us/step - loss: 0.1831 - val_loss: 0.0723\n",
            "Epoch 307/500\n",
            "358/358 [==============================] - 0s 61us/step - loss: 0.1636 - val_loss: 0.1250\n",
            "Epoch 308/500\n",
            "358/358 [==============================] - 0s 61us/step - loss: 0.1503 - val_loss: 0.3845\n",
            "Epoch 309/500\n",
            "358/358 [==============================] - 0s 63us/step - loss: 0.1528 - val_loss: 0.1678\n",
            "Epoch 310/500\n",
            "358/358 [==============================] - 0s 55us/step - loss: 0.1629 - val_loss: 0.1251\n",
            "Epoch 311/500\n",
            "358/358 [==============================] - 0s 59us/step - loss: 0.1463 - val_loss: 0.0764\n",
            "Epoch 312/500\n",
            "358/358 [==============================] - 0s 59us/step - loss: 0.1681 - val_loss: 0.4430\n",
            "Epoch 313/500\n",
            "358/358 [==============================] - 0s 58us/step - loss: 0.1787 - val_loss: 0.0572\n",
            "Epoch 314/500\n",
            "358/358 [==============================] - 0s 58us/step - loss: 0.1631 - val_loss: 0.2735\n",
            "Epoch 315/500\n",
            "358/358 [==============================] - 0s 60us/step - loss: 0.1559 - val_loss: 0.0747\n",
            "Epoch 316/500\n",
            "358/358 [==============================] - 0s 59us/step - loss: 0.1521 - val_loss: 0.0813\n",
            "Epoch 317/500\n",
            "358/358 [==============================] - 0s 64us/step - loss: 0.1589 - val_loss: 0.0964\n",
            "Epoch 318/500\n",
            "358/358 [==============================] - 0s 68us/step - loss: 0.1575 - val_loss: 0.4591\n",
            "Epoch 319/500\n",
            "358/358 [==============================] - 0s 78us/step - loss: 0.1600 - val_loss: 0.1081\n",
            "Epoch 320/500\n",
            "358/358 [==============================] - 0s 74us/step - loss: 0.1505 - val_loss: 0.0782\n",
            "Epoch 321/500\n",
            "358/358 [==============================] - 0s 76us/step - loss: 0.1663 - val_loss: 0.2390\n",
            "Epoch 322/500\n",
            "358/358 [==============================] - 0s 76us/step - loss: 0.1516 - val_loss: 0.0744\n",
            "Epoch 323/500\n",
            "358/358 [==============================] - 0s 76us/step - loss: 0.1648 - val_loss: 0.1811\n",
            "Epoch 324/500\n",
            "358/358 [==============================] - 0s 79us/step - loss: 0.1560 - val_loss: 0.1076\n",
            "Epoch 325/500\n",
            "358/358 [==============================] - 0s 98us/step - loss: 0.1625 - val_loss: 0.2694\n",
            "Epoch 326/500\n",
            "358/358 [==============================] - 0s 76us/step - loss: 0.1683 - val_loss: 0.0486\n",
            "Epoch 327/500\n",
            "358/358 [==============================] - 0s 71us/step - loss: 0.1647 - val_loss: 0.1115\n",
            "Epoch 328/500\n",
            "358/358 [==============================] - 0s 72us/step - loss: 0.1470 - val_loss: 0.1176\n",
            "Epoch 329/500\n",
            "358/358 [==============================] - 0s 77us/step - loss: 0.1657 - val_loss: 0.2857\n",
            "Epoch 330/500\n",
            "358/358 [==============================] - 0s 75us/step - loss: 0.1575 - val_loss: 0.0603\n",
            "Epoch 331/500\n",
            "358/358 [==============================] - 0s 74us/step - loss: 0.1442 - val_loss: 0.0730\n",
            "Epoch 332/500\n",
            "358/358 [==============================] - 0s 80us/step - loss: 0.1597 - val_loss: 0.0448\n",
            "Epoch 333/500\n",
            "358/358 [==============================] - 0s 72us/step - loss: 0.1549 - val_loss: 0.4170\n",
            "Epoch 334/500\n",
            "358/358 [==============================] - 0s 75us/step - loss: 0.1581 - val_loss: 0.1663\n",
            "Epoch 335/500\n",
            "358/358 [==============================] - 0s 73us/step - loss: 0.1492 - val_loss: 0.2856\n",
            "Epoch 336/500\n",
            "358/358 [==============================] - 0s 74us/step - loss: 0.1690 - val_loss: 0.0697\n",
            "Epoch 337/500\n",
            "358/358 [==============================] - 0s 73us/step - loss: 0.1599 - val_loss: 0.1324\n",
            "Epoch 338/500\n",
            "358/358 [==============================] - 0s 76us/step - loss: 0.1554 - val_loss: 0.4618\n",
            "Epoch 339/500\n",
            "358/358 [==============================] - 0s 80us/step - loss: 0.1770 - val_loss: 0.2655\n",
            "Epoch 340/500\n",
            "358/358 [==============================] - 0s 69us/step - loss: 0.1598 - val_loss: 0.1210\n",
            "Epoch 341/500\n",
            "358/358 [==============================] - 0s 73us/step - loss: 0.1457 - val_loss: 0.0970\n",
            "Epoch 342/500\n",
            "358/358 [==============================] - 0s 73us/step - loss: 0.1552 - val_loss: 0.2666\n",
            "Epoch 343/500\n",
            "358/358 [==============================] - 0s 83us/step - loss: 0.1573 - val_loss: 0.0435\n",
            "Epoch 344/500\n",
            "358/358 [==============================] - 0s 79us/step - loss: 0.1750 - val_loss: 0.3741\n",
            "Epoch 345/500\n",
            "358/358 [==============================] - 0s 69us/step - loss: 0.1630 - val_loss: 0.0862\n",
            "Epoch 346/500\n",
            "358/358 [==============================] - 0s 55us/step - loss: 0.1578 - val_loss: 0.0922\n",
            "Epoch 347/500\n",
            "358/358 [==============================] - 0s 56us/step - loss: 0.1465 - val_loss: 0.2567\n",
            "Epoch 348/500\n",
            "358/358 [==============================] - 0s 57us/step - loss: 0.1532 - val_loss: 0.1094\n",
            "Epoch 349/500\n",
            "358/358 [==============================] - 0s 58us/step - loss: 0.1491 - val_loss: 0.0548\n",
            "Epoch 350/500\n",
            "358/358 [==============================] - 0s 60us/step - loss: 0.1516 - val_loss: 0.0690\n",
            "Epoch 351/500\n",
            "358/358 [==============================] - 0s 67us/step - loss: 0.1540 - val_loss: 0.1871\n",
            "Epoch 352/500\n",
            "358/358 [==============================] - 0s 64us/step - loss: 0.1500 - val_loss: 0.2834\n",
            "Epoch 353/500\n",
            "358/358 [==============================] - 0s 60us/step - loss: 0.1502 - val_loss: 0.0497\n",
            "Epoch 354/500\n",
            "358/358 [==============================] - 0s 64us/step - loss: 0.1740 - val_loss: 0.3624\n",
            "Epoch 355/500\n",
            "358/358 [==============================] - 0s 59us/step - loss: 0.1600 - val_loss: 0.1700\n",
            "Epoch 356/500\n",
            "358/358 [==============================] - 0s 68us/step - loss: 0.1488 - val_loss: 0.1098\n",
            "Epoch 357/500\n",
            "358/358 [==============================] - 0s 60us/step - loss: 0.1591 - val_loss: 0.1428\n",
            "Epoch 358/500\n",
            "358/358 [==============================] - 0s 68us/step - loss: 0.1585 - val_loss: 0.1253\n",
            "Epoch 359/500\n",
            "358/358 [==============================] - 0s 61us/step - loss: 0.1464 - val_loss: 0.0754\n",
            "Epoch 360/500\n",
            "358/358 [==============================] - 0s 58us/step - loss: 0.1632 - val_loss: 0.1404\n",
            "Epoch 361/500\n",
            "358/358 [==============================] - 0s 59us/step - loss: 0.1553 - val_loss: 0.1064\n",
            "Epoch 362/500\n",
            "358/358 [==============================] - 0s 64us/step - loss: 0.1670 - val_loss: 0.0638\n",
            "Epoch 363/500\n",
            "358/358 [==============================] - 0s 59us/step - loss: 0.1563 - val_loss: 0.2162\n",
            "Epoch 364/500\n",
            "358/358 [==============================] - 0s 59us/step - loss: 0.1557 - val_loss: 0.0418\n",
            "Epoch 365/500\n",
            "358/358 [==============================] - 0s 61us/step - loss: 0.1647 - val_loss: 0.0596\n",
            "Epoch 366/500\n",
            "358/358 [==============================] - 0s 60us/step - loss: 0.1455 - val_loss: 0.1465\n",
            "Epoch 367/500\n",
            "358/358 [==============================] - 0s 59us/step - loss: 0.1341 - val_loss: 0.1846\n",
            "Epoch 368/500\n",
            "358/358 [==============================] - 0s 59us/step - loss: 0.1565 - val_loss: 0.0380\n",
            "Epoch 369/500\n",
            "358/358 [==============================] - 0s 68us/step - loss: 0.1822 - val_loss: 0.1509\n",
            "Epoch 370/500\n",
            "358/358 [==============================] - 0s 57us/step - loss: 0.1471 - val_loss: 0.2894\n",
            "Epoch 371/500\n",
            "358/358 [==============================] - 0s 55us/step - loss: 0.1545 - val_loss: 0.1579\n",
            "Epoch 372/500\n",
            "358/358 [==============================] - 0s 62us/step - loss: 0.1587 - val_loss: 0.0795\n",
            "Epoch 373/500\n",
            "358/358 [==============================] - 0s 59us/step - loss: 0.1447 - val_loss: 0.1312\n",
            "Epoch 374/500\n",
            "358/358 [==============================] - 0s 62us/step - loss: 0.1497 - val_loss: 0.1772\n",
            "Epoch 375/500\n",
            "358/358 [==============================] - 0s 62us/step - loss: 0.1366 - val_loss: 0.2148\n",
            "Epoch 376/500\n",
            "358/358 [==============================] - 0s 59us/step - loss: 0.1578 - val_loss: 0.2450\n",
            "Epoch 377/500\n",
            "358/358 [==============================] - 0s 61us/step - loss: 0.1675 - val_loss: 0.0443\n",
            "Epoch 378/500\n",
            "358/358 [==============================] - 0s 69us/step - loss: 0.1604 - val_loss: 0.1295\n",
            "Epoch 379/500\n",
            "358/358 [==============================] - 0s 58us/step - loss: 0.1466 - val_loss: 0.1583\n",
            "Epoch 380/500\n",
            "358/358 [==============================] - 0s 59us/step - loss: 0.1462 - val_loss: 0.2264\n",
            "Epoch 381/500\n",
            "358/358 [==============================] - 0s 57us/step - loss: 0.1441 - val_loss: 0.0437\n",
            "Epoch 382/500\n",
            "358/358 [==============================] - 0s 64us/step - loss: 0.1760 - val_loss: 0.4076\n",
            "Epoch 383/500\n",
            "358/358 [==============================] - 0s 59us/step - loss: 0.1574 - val_loss: 0.0513\n",
            "Epoch 384/500\n",
            "358/358 [==============================] - 0s 58us/step - loss: 0.1559 - val_loss: 0.2106\n",
            "Epoch 385/500\n",
            "358/358 [==============================] - 0s 64us/step - loss: 0.1365 - val_loss: 0.2210\n",
            "Epoch 386/500\n",
            "358/358 [==============================] - 0s 60us/step - loss: 0.1535 - val_loss: 0.2703\n",
            "Epoch 387/500\n",
            "358/358 [==============================] - 0s 65us/step - loss: 0.1592 - val_loss: 0.1753\n",
            "Epoch 388/500\n",
            "358/358 [==============================] - 0s 59us/step - loss: 0.1447 - val_loss: 0.1245\n",
            "Epoch 389/500\n",
            "358/358 [==============================] - 0s 62us/step - loss: 0.1595 - val_loss: 0.0695\n",
            "Epoch 390/500\n",
            "358/358 [==============================] - 0s 58us/step - loss: 0.1361 - val_loss: 0.2102\n",
            "Epoch 391/500\n",
            "358/358 [==============================] - 0s 60us/step - loss: 0.1603 - val_loss: 0.1220\n",
            "Epoch 392/500\n",
            "358/358 [==============================] - 0s 61us/step - loss: 0.1437 - val_loss: 0.0807\n",
            "Epoch 393/500\n",
            "358/358 [==============================] - 0s 59us/step - loss: 0.1587 - val_loss: 0.1442\n",
            "Epoch 394/500\n",
            "358/358 [==============================] - 0s 59us/step - loss: 0.1565 - val_loss: 0.1306\n",
            "Epoch 395/500\n",
            "358/358 [==============================] - 0s 74us/step - loss: 0.1453 - val_loss: 0.0440\n",
            "Epoch 396/500\n",
            "358/358 [==============================] - 0s 76us/step - loss: 0.1564 - val_loss: 0.0620\n",
            "Epoch 397/500\n",
            "358/358 [==============================] - 0s 67us/step - loss: 0.1569 - val_loss: 0.1502\n",
            "Epoch 398/500\n",
            "358/358 [==============================] - 0s 60us/step - loss: 0.1498 - val_loss: 0.0888\n",
            "Epoch 399/500\n",
            "358/358 [==============================] - 0s 60us/step - loss: 0.1435 - val_loss: 0.2986\n",
            "Epoch 400/500\n",
            "358/358 [==============================] - 0s 66us/step - loss: 0.1779 - val_loss: 0.1320\n",
            "Epoch 401/500\n",
            "358/358 [==============================] - 0s 57us/step - loss: 0.1421 - val_loss: 0.3916\n",
            "Epoch 402/500\n",
            "358/358 [==============================] - 0s 58us/step - loss: 0.1481 - val_loss: 0.0836\n",
            "Epoch 403/500\n",
            "358/358 [==============================] - 0s 54us/step - loss: 0.1532 - val_loss: 0.3233\n",
            "Epoch 404/500\n",
            "358/358 [==============================] - 0s 56us/step - loss: 0.1561 - val_loss: 0.2003\n",
            "Epoch 405/500\n",
            "358/358 [==============================] - 0s 58us/step - loss: 0.1517 - val_loss: 0.0457\n",
            "Epoch 406/500\n",
            "358/358 [==============================] - 0s 67us/step - loss: 0.1490 - val_loss: 0.3726\n",
            "Epoch 407/500\n",
            "358/358 [==============================] - 0s 62us/step - loss: 0.1508 - val_loss: 0.2511\n",
            "Epoch 408/500\n",
            "358/358 [==============================] - 0s 63us/step - loss: 0.1386 - val_loss: 0.2508\n",
            "Epoch 409/500\n",
            "358/358 [==============================] - 0s 71us/step - loss: 0.1582 - val_loss: 0.1550\n",
            "Epoch 410/500\n",
            "358/358 [==============================] - 0s 74us/step - loss: 0.1579 - val_loss: 0.0682\n",
            "Epoch 411/500\n",
            "358/358 [==============================] - 0s 79us/step - loss: 0.1369 - val_loss: 0.1465\n",
            "Epoch 412/500\n",
            "358/358 [==============================] - 0s 87us/step - loss: 0.1574 - val_loss: 0.4304\n",
            "Epoch 413/500\n",
            "358/358 [==============================] - 0s 95us/step - loss: 0.1669 - val_loss: 0.1720\n",
            "Epoch 414/500\n",
            "358/358 [==============================] - 0s 81us/step - loss: 0.1321 - val_loss: 0.2785\n",
            "Epoch 415/500\n",
            "358/358 [==============================] - 0s 79us/step - loss: 0.1660 - val_loss: 0.1117\n",
            "Epoch 416/500\n",
            "358/358 [==============================] - 0s 79us/step - loss: 0.1530 - val_loss: 0.0603\n",
            "Epoch 417/500\n",
            "358/358 [==============================] - 0s 77us/step - loss: 0.1478 - val_loss: 0.0542\n",
            "Epoch 418/500\n",
            "358/358 [==============================] - 0s 71us/step - loss: 0.1424 - val_loss: 0.0777\n",
            "Epoch 419/500\n",
            "358/358 [==============================] - 0s 57us/step - loss: 0.1542 - val_loss: 0.1484\n",
            "Epoch 420/500\n",
            "358/358 [==============================] - 0s 61us/step - loss: 0.1656 - val_loss: 0.2911\n",
            "Epoch 421/500\n",
            "358/358 [==============================] - 0s 58us/step - loss: 0.1441 - val_loss: 0.3405\n",
            "Epoch 422/500\n",
            "358/358 [==============================] - 0s 57us/step - loss: 0.1543 - val_loss: 0.0812\n",
            "Epoch 423/500\n",
            "358/358 [==============================] - 0s 64us/step - loss: 0.1581 - val_loss: 0.0654\n",
            "Epoch 424/500\n",
            "358/358 [==============================] - 0s 56us/step - loss: 0.1523 - val_loss: 0.1843\n",
            "Epoch 425/500\n",
            "358/358 [==============================] - 0s 56us/step - loss: 0.1356 - val_loss: 0.1218\n",
            "Epoch 426/500\n",
            "358/358 [==============================] - 0s 56us/step - loss: 0.1315 - val_loss: 0.1308\n",
            "Epoch 427/500\n",
            "358/358 [==============================] - 0s 57us/step - loss: 0.1379 - val_loss: 0.0561\n",
            "Epoch 428/500\n",
            "358/358 [==============================] - 0s 56us/step - loss: 0.1387 - val_loss: 0.0449\n",
            "Epoch 429/500\n",
            "358/358 [==============================] - 0s 59us/step - loss: 0.1558 - val_loss: 0.2800\n",
            "Epoch 430/500\n",
            "358/358 [==============================] - 0s 57us/step - loss: 0.1480 - val_loss: 0.0729\n",
            "Epoch 431/500\n",
            "358/358 [==============================] - 0s 58us/step - loss: 0.1450 - val_loss: 0.1637\n",
            "Epoch 432/500\n",
            "358/358 [==============================] - 0s 60us/step - loss: 0.1438 - val_loss: 0.0735\n",
            "Epoch 433/500\n",
            "358/358 [==============================] - 0s 67us/step - loss: 0.1396 - val_loss: 0.0975\n",
            "Epoch 434/500\n",
            "358/358 [==============================] - 0s 61us/step - loss: 0.1457 - val_loss: 0.2141\n",
            "Epoch 435/500\n",
            "358/358 [==============================] - 0s 62us/step - loss: 0.1231 - val_loss: 0.0343\n",
            "Epoch 436/500\n",
            "358/358 [==============================] - 0s 59us/step - loss: 0.1459 - val_loss: 0.0754\n",
            "Epoch 437/500\n",
            "358/358 [==============================] - 0s 58us/step - loss: 0.1503 - val_loss: 0.0399\n",
            "Epoch 438/500\n",
            "358/358 [==============================] - 0s 59us/step - loss: 0.1592 - val_loss: 0.3136\n",
            "Epoch 439/500\n",
            "358/358 [==============================] - 0s 58us/step - loss: 0.1424 - val_loss: 0.3843\n",
            "Epoch 440/500\n",
            "358/358 [==============================] - 0s 58us/step - loss: 0.1576 - val_loss: 0.0563\n",
            "Epoch 441/500\n",
            "358/358 [==============================] - 0s 58us/step - loss: 0.1548 - val_loss: 0.0537\n",
            "Epoch 442/500\n",
            "358/358 [==============================] - 0s 66us/step - loss: 0.1444 - val_loss: 0.0554\n",
            "Epoch 443/500\n",
            "358/358 [==============================] - 0s 68us/step - loss: 0.1372 - val_loss: 0.0500\n",
            "Epoch 444/500\n",
            "358/358 [==============================] - 0s 64us/step - loss: 0.1436 - val_loss: 0.1777\n",
            "Epoch 445/500\n",
            "358/358 [==============================] - 0s 60us/step - loss: 0.1393 - val_loss: 0.5671\n",
            "Epoch 446/500\n",
            "358/358 [==============================] - 0s 56us/step - loss: 0.1508 - val_loss: 0.3067\n",
            "Epoch 447/500\n",
            "358/358 [==============================] - 0s 58us/step - loss: 0.1381 - val_loss: 0.1733\n",
            "Epoch 448/500\n",
            "358/358 [==============================] - 0s 57us/step - loss: 0.1461 - val_loss: 0.1136\n",
            "Epoch 449/500\n",
            "358/358 [==============================] - 0s 57us/step - loss: 0.1462 - val_loss: 0.0699\n",
            "Epoch 450/500\n",
            "358/358 [==============================] - 0s 69us/step - loss: 0.1411 - val_loss: 0.1084\n",
            "Epoch 451/500\n",
            "358/358 [==============================] - 0s 58us/step - loss: 0.1532 - val_loss: 0.2485\n",
            "Epoch 452/500\n",
            "358/358 [==============================] - 0s 56us/step - loss: 0.1652 - val_loss: 0.2699\n",
            "Epoch 453/500\n",
            "358/358 [==============================] - 0s 59us/step - loss: 0.1552 - val_loss: 0.0668\n",
            "Epoch 454/500\n",
            "358/358 [==============================] - 0s 65us/step - loss: 0.1523 - val_loss: 0.2569\n",
            "Epoch 455/500\n",
            "358/358 [==============================] - 0s 78us/step - loss: 0.1383 - val_loss: 0.0445\n",
            "Epoch 456/500\n",
            "358/358 [==============================] - 0s 74us/step - loss: 0.1622 - val_loss: 0.1021\n",
            "Epoch 457/500\n",
            "358/358 [==============================] - 0s 80us/step - loss: 0.1343 - val_loss: 0.2406\n",
            "Epoch 458/500\n",
            "358/358 [==============================] - 0s 77us/step - loss: 0.1557 - val_loss: 0.0459\n",
            "Epoch 459/500\n",
            "358/358 [==============================] - 0s 74us/step - loss: 0.1576 - val_loss: 0.1398\n",
            "Epoch 460/500\n",
            "358/358 [==============================] - 0s 73us/step - loss: 0.1487 - val_loss: 0.1456\n",
            "Epoch 461/500\n",
            "358/358 [==============================] - 0s 75us/step - loss: 0.1366 - val_loss: 0.1017\n",
            "Epoch 462/500\n",
            "358/358 [==============================] - 0s 70us/step - loss: 0.1579 - val_loss: 0.0424\n",
            "Epoch 463/500\n",
            "358/358 [==============================] - 0s 75us/step - loss: 0.1416 - val_loss: 0.1948\n",
            "Epoch 464/500\n",
            "358/358 [==============================] - 0s 76us/step - loss: 0.1411 - val_loss: 0.1198\n",
            "Epoch 465/500\n",
            "358/358 [==============================] - 0s 70us/step - loss: 0.1525 - val_loss: 0.0963\n",
            "Epoch 466/500\n",
            "358/358 [==============================] - 0s 74us/step - loss: 0.1252 - val_loss: 0.3930\n",
            "Epoch 467/500\n",
            "358/358 [==============================] - 0s 75us/step - loss: 0.1463 - val_loss: 0.0737\n",
            "Epoch 468/500\n",
            "358/358 [==============================] - 0s 73us/step - loss: 0.1457 - val_loss: 0.4069\n",
            "Epoch 469/500\n",
            "358/358 [==============================] - 0s 73us/step - loss: 0.1454 - val_loss: 0.0545\n",
            "Epoch 470/500\n",
            "358/358 [==============================] - 0s 72us/step - loss: 0.1379 - val_loss: 0.2972\n",
            "Epoch 471/500\n",
            "358/358 [==============================] - 0s 84us/step - loss: 0.1600 - val_loss: 0.4987\n",
            "Epoch 472/500\n",
            "358/358 [==============================] - 0s 72us/step - loss: 0.1545 - val_loss: 0.0692\n",
            "Epoch 473/500\n",
            "358/358 [==============================] - 0s 80us/step - loss: 0.1364 - val_loss: 0.3432\n",
            "Epoch 474/500\n",
            "358/358 [==============================] - 0s 74us/step - loss: 0.1341 - val_loss: 0.2416\n",
            "Epoch 475/500\n",
            "358/358 [==============================] - 0s 71us/step - loss: 0.1491 - val_loss: 0.0335\n",
            "Epoch 476/500\n",
            "358/358 [==============================] - 0s 75us/step - loss: 0.1549 - val_loss: 0.0720\n",
            "Epoch 477/500\n",
            "358/358 [==============================] - 0s 75us/step - loss: 0.1386 - val_loss: 0.4733\n",
            "Epoch 478/500\n",
            "358/358 [==============================] - 0s 75us/step - loss: 0.1526 - val_loss: 0.3207\n",
            "Epoch 479/500\n",
            "358/358 [==============================] - 0s 73us/step - loss: 0.1443 - val_loss: 0.1282\n",
            "Epoch 480/500\n",
            "358/358 [==============================] - 0s 77us/step - loss: 0.1451 - val_loss: 0.1004\n",
            "Epoch 481/500\n",
            "358/358 [==============================] - 0s 95us/step - loss: 0.1576 - val_loss: 0.2537\n",
            "Epoch 482/500\n",
            "358/358 [==============================] - 0s 70us/step - loss: 0.1452 - val_loss: 0.1415\n",
            "Epoch 483/500\n",
            "358/358 [==============================] - 0s 84us/step - loss: 0.1411 - val_loss: 0.0581\n",
            "Epoch 484/500\n",
            "358/358 [==============================] - 0s 76us/step - loss: 0.1418 - val_loss: 0.1011\n",
            "Epoch 485/500\n",
            "358/358 [==============================] - 0s 77us/step - loss: 0.1378 - val_loss: 0.3551\n",
            "Epoch 486/500\n",
            "358/358 [==============================] - 0s 83us/step - loss: 0.1398 - val_loss: 0.0642\n",
            "Epoch 487/500\n",
            "358/358 [==============================] - 0s 94us/step - loss: 0.1496 - val_loss: 0.1469\n",
            "Epoch 488/500\n",
            "358/358 [==============================] - 0s 80us/step - loss: 0.1378 - val_loss: 0.5998\n",
            "Epoch 489/500\n",
            "358/358 [==============================] - 0s 82us/step - loss: 0.1587 - val_loss: 0.0474\n",
            "Epoch 490/500\n",
            "358/358 [==============================] - 0s 81us/step - loss: 0.1437 - val_loss: 0.0645\n",
            "Epoch 491/500\n",
            "358/358 [==============================] - 0s 78us/step - loss: 0.1423 - val_loss: 0.0754\n",
            "Epoch 492/500\n",
            "358/358 [==============================] - 0s 80us/step - loss: 0.1308 - val_loss: 0.1116\n",
            "Epoch 493/500\n",
            "358/358 [==============================] - 0s 84us/step - loss: 0.1433 - val_loss: 0.1037\n",
            "Epoch 494/500\n",
            "358/358 [==============================] - 0s 79us/step - loss: 0.1476 - val_loss: 0.2778\n",
            "Epoch 495/500\n",
            "358/358 [==============================] - 0s 79us/step - loss: 0.1269 - val_loss: 0.0812\n",
            "Epoch 496/500\n",
            "358/358 [==============================] - 0s 77us/step - loss: 0.1312 - val_loss: 0.2808\n",
            "Epoch 497/500\n",
            "358/358 [==============================] - 0s 59us/step - loss: 0.1228 - val_loss: 0.2070\n",
            "Epoch 498/500\n",
            "358/358 [==============================] - 0s 57us/step - loss: 0.1625 - val_loss: 0.1762\n",
            "Epoch 499/500\n",
            "358/358 [==============================] - 0s 59us/step - loss: 0.1277 - val_loss: 0.0548\n",
            "Epoch 500/500\n",
            "358/358 [==============================] - 0s 61us/step - loss: 0.1375 - val_loss: 0.0459\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f3c3fb0ccf8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcXD_dMB0iOI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "a285298c-6016-416f-da8c-658f47c7d130"
      },
      "source": [
        "print('Generating test predictions...')\n",
        "preds = mlp_model.predict_classes(X_test_scale, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generating test predictions...\n",
            "171/171 [==============================] - 0s 74us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "setSm3rX3n22",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "1bb803bd-4196-4447-ae84-a496e5a6accd"
      },
      "source": [
        "preds"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
              "       1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,\n",
              "       1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,\n",
              "       1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
              "       1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,\n",
              "       1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,\n",
              "       0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,\n",
              "       1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9ytXq-d2LlC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "226683f5-dab1-4e2f-acaa-f596a0d34691"
      },
      "source": [
        "yyy = mlp_model.predict(X_test_scale, verbose=1)\n",
        "print(yyy[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "171/171 [==============================] - 0s 29us/step\n",
            "[0.00456812 0.9954319 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9pWkDSX2fbh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "ac8d70a1-79e6-4c6f-c531-eaaf89722746"
      },
      "source": [
        "print(np.argmax(yyy, axis=1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 1 0 0 1 0 1 1 1 0 0 0 1 0 1 1 0 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1\n",
            " 0 1 1 0 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 1 1 1 0 0 1 0 1 0 0 1 1 1 1 0 1 1 1\n",
            " 0 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 1 0 1 1\n",
            " 0 1 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 0 0 0 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0\n",
            " 0 0 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 1 1 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZMry5jh7UIB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d2b6fd7a-2bde-480c-8f03-a37119854e53"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "print(accuracy_score(y_test, preds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9473684210526315\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khbVFUaF-fwH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "f8c20faf-3972-44be-eb45-26ed1821400b"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "print(classification_report(y_test,preds))\n",
        "print('Confusion matrix')\n",
        "print(confusion_matrix(y_test,preds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.90      0.93        63\n",
            "           1       0.95      0.97      0.96       108\n",
            "\n",
            "    accuracy                           0.95       171\n",
            "   macro avg       0.95      0.94      0.94       171\n",
            "weighted avg       0.95      0.95      0.95       171\n",
            "\n",
            "Confusion matrix\n",
            "[[ 57   6]\n",
            " [  3 105]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNkuJ03A-oYe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}